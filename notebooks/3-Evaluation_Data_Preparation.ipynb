{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import os\n",
    "os.environ[\"LANGUAGE\"] = 'ar'\n",
    "\n",
    "from sqlalchemy.sql.expression import func\n",
    "from wikidataDB import WikidataEntity, WikidataID, Session\n",
    "from wikidataRetriever import WikidataKeywordSearch, AstraDBConnect\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pickle\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "from requests.exceptions import HTTPError\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def is_in_wikipedia(qid):\n",
    "    item = WikidataID.get_id(qid)\n",
    "    if item is None:\n",
    "        return False\n",
    "    return item.in_wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a single file\n",
    "def process_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    processed_rows = []\n",
    "    for _, data_chunks in data.items():\n",
    "        for data_chunk in data_chunks:\n",
    "            triple = data_chunk['triple']\n",
    "            question_in_wikipedia = is_in_wikipedia(triple[0])\n",
    "            answer_in_wikipedia = is_in_wikipedia(triple[2])\n",
    "            question = data_chunk['question variants'][0]\n",
    "            processed_rows.append({\n",
    "                'Question QID': triple[0],\n",
    "                'Property PID': triple[1],\n",
    "                'Answer QID': triple[2],\n",
    "                'Question in Wikipedia': question_in_wikipedia,\n",
    "                'Answer in Wikipedia': answer_in_wikipedia,\n",
    "                'Question': question['out-of-context'],\n",
    "                'Answer': data_chunk['answer']\n",
    "            })\n",
    "    return processed_rows\n",
    "\n",
    "# Use a list to accumulate the rows\n",
    "all_data = []\n",
    "\n",
    "main_dir = \"../data/Evaluation Data/KGConv/complete_version\"\n",
    "\n",
    "# Using ThreadPoolExecutor to parallelize file processing\n",
    "with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "    futures = []\n",
    "    for folder in os.listdir(main_dir):\n",
    "        current_dir = os.path.join(main_dir, folder)\n",
    "        for file in tqdm(os.listdir(current_dir)):\n",
    "            file_path = os.path.join(current_dir, file)\n",
    "            futures.append(executor.submit(process_file, file_path))\n",
    "\n",
    "    for future in tqdm(futures):\n",
    "        all_data.extend(future.result())\n",
    "\n",
    "# Convert to DataFrame all at once at the end\n",
    "clean_data = pd.DataFrame(all_data)\n",
    "pickle.dump(clean_data, open(\"../data/Evaluation Data/KGConv/processed_dataframe.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a single file\n",
    "def process_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    processed_rows = []\n",
    "    for data_chunk in data:\n",
    "        try:\n",
    "            question_qids = [d['name'] for d in data_chunk['questionEntity'] if d['entityType'] == 'entity']\n",
    "            question_in_wikipedia = [is_in_wikipedia(id) for id in question_qids]\n",
    "\n",
    "            answer_type = data_chunk['answer']['answerType']\n",
    "\n",
    "            answer_qids = []\n",
    "            answer_in_wikipedia = []\n",
    "            if (answer_type == 'entity') and (data_chunk['answer']['answer'] is not None):\n",
    "                answer_qids = [d['name'] for d in data_chunk['answer']['answer']]\n",
    "                answer_in_wikipedia = [is_in_wikipedia(id) for id in answer_qids]\n",
    "\n",
    "            processed_rows.append({\n",
    "                'Question QIDs': question_qids,\n",
    "                'Answer QIDs': answer_qids,\n",
    "                'Question in Wikipedia': question_in_wikipedia,\n",
    "                'Answer in Wikipedia': answer_in_wikipedia,\n",
    "                'Question': data_chunk['question'],\n",
    "                'Answer': data_chunk['answer']['mention'],\n",
    "                'Answer Type': answer_type,\n",
    "                'Language': 'en'\n",
    "            })\n",
    "\n",
    "            for lang in data_chunk['translations'].keys():\n",
    "                processed_rows.append({\n",
    "                    'Question QIDs': question_qids,\n",
    "                    'Answer QIDs': answer_qids,\n",
    "                    'Question in Wikipedia': question_in_wikipedia,\n",
    "                    'Answer in Wikipedia': answer_in_wikipedia,\n",
    "                    'Question': data_chunk['translations'][lang],\n",
    "                    'Answer': data_chunk['answer']['mention'],\n",
    "                    'Answer Type': answer_type,\n",
    "                    'Language': lang\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(data_chunk)\n",
    "            raise e\n",
    "    return processed_rows\n",
    "\n",
    "# Use a list to accumulate the rows\n",
    "all_data = []\n",
    "\n",
    "main_dir = \"../data/Evaluation Data/Mintaka\"\n",
    "\n",
    "# Using ThreadPoolExecutor to parallelize file processing\n",
    "with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "    futures = []\n",
    "    for file in tqdm(os.listdir(main_dir)):\n",
    "        if 'json' in file:\n",
    "            file_path = os.path.join(main_dir, file)\n",
    "            futures.append(executor.submit(process_file, file_path))\n",
    "\n",
    "    for future in tqdm(futures):\n",
    "        all_data.extend(future.result())\n",
    "\n",
    "# Convert to DataFrame all at once at the end\n",
    "clean_data = pd.DataFrame(all_data)\n",
    "pickle.dump(clean_data, open(\"../data/Evaluation Data/Mintaka/processed_dataframe_full.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a single file\n",
    "def process_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    processed_rows = []\n",
    "    for data_chunk in data:\n",
    "        try:\n",
    "            if data_chunk['question_uris'] is not None:\n",
    "                question_qids = [d.split('/')[-1] for d in data_chunk['question_uris']]\n",
    "                question_in_wikipedia = [is_in_wikipedia(id) for id in question_qids]\n",
    "\n",
    "                answers = [d['wd_names']['en'][0] if len(d['wd_names']['en']) > 0 else d['label'] for d in data_chunk['answers']]\n",
    "                answer_qids = [d['value'].split('/')[-1] for d in data_chunk['answers']]\n",
    "                answer_in_wikipedia = [is_in_wikipedia(id) for id in answer_qids]\n",
    "\n",
    "                processed_rows.append({\n",
    "                    'Question QIDs': question_qids,\n",
    "                    'Answer QIDs': answer_qids,\n",
    "                    'Question in Wikipedia': question_in_wikipedia,\n",
    "                    'Answer in Wikipedia': answer_in_wikipedia,\n",
    "                    'Question': data_chunk['question_eng'],\n",
    "                    'Answer': answers,\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(data_chunk)\n",
    "            raise e\n",
    "    return processed_rows\n",
    "\n",
    "# Use a list to accumulate the rows\n",
    "all_data = []\n",
    "\n",
    "main_dir = \"../data/Evaluation Data/RuBQ\"\n",
    "\n",
    "# Using ThreadPoolExecutor to parallelize file processing\n",
    "with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "    futures = []\n",
    "    for file in tqdm(os.listdir(main_dir)):\n",
    "        file_path = os.path.join(main_dir, file)\n",
    "        futures.append(executor.submit(process_file, file_path))\n",
    "\n",
    "    for future in tqdm(futures):\n",
    "        all_data.extend(future.result())\n",
    "\n",
    "# Convert to DataFrame all at once at the end\n",
    "clean_data = pd.DataFrame(all_data)\n",
    "pickle.dump(clean_data, open(\"../data/Evaluation Data/RuBQ/processed_dataframe.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidata_endpoint = \"https://query.wikidata.org/sparql\"\n",
    "sparql = SPARQLWrapper(wikidata_endpoint)\n",
    "sparql.setReturnFormat(JSON)\n",
    "\n",
    "sparql.setQuery(\"SELECT DISTINCT ?sbj ?sbj_label WHERE { ?sbj wdt:P31 wd:Q7187 . ?sbj rdfs:label ?sbj_label . FILTER(CONTAINS(lcase(?sbj_label), 'vgf')) . FILTER (lang(?sbj_label) = 'en') } LIMIT 25\")\n",
    "results = sparql.query().convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests.exceptions import HTTPError\n",
    "import time\n",
    "\n",
    "# Function to process a single file\n",
    "def process_file(file_path):\n",
    "    wikidata_endpoint = \"https://query.wikidata.org/sparql\"\n",
    "    sparql = SPARQLWrapper(wikidata_endpoint)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    processed_rows = []\n",
    "    for data_chunk in data:\n",
    "        try:\n",
    "            matches = re.findall(r'wd:Q\\d+', data_chunk['sparql_wikidata'])\n",
    "            question_qids = [match[3:] for match in matches]\n",
    "            question_in_wikipedia = [is_in_wikipedia(id) for id in question_qids]\n",
    "\n",
    "            answer_qids = []\n",
    "            query = data_chunk['sparql_wikidata']\n",
    "            ran_sparql = True\n",
    "            if not data_chunk['sparql_wikidata'].lower().strip().startswith('ask'):\n",
    "                results = []\n",
    "                retry = 5\n",
    "                ran_sparql = False\n",
    "                while retry > 0:  # Retry up to 5 times\n",
    "                    try:\n",
    "                        sparql.setQuery(query)\n",
    "                        results = sparql.query().convert()[\"results\"][\"bindings\"]\n",
    "                        retry = 0  # Exit loop if successful\n",
    "                        ran_sparql = True\n",
    "                    except HTTPError as e:\n",
    "                        print(e)\n",
    "                        retry -= 1\n",
    "                        time.sleep(1)\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        retry -= 1\n",
    "                        query = re.sub(r'LIMIT \\d+', 'LIMIT 5', query, flags=re.IGNORECASE)\n",
    "\n",
    "                for result in results:\n",
    "                    for key in result:\n",
    "                        value = result[key][\"value\"]\n",
    "                        if 'www.wikidata.org' in result[key][\"value\"]:\n",
    "                            answer_qids.append(value.split('/')[-1])\n",
    "                answer_in_wikipedia = [is_in_wikipedia(id) for id in answer_qids]\n",
    "\n",
    "            processed_rows.append({\n",
    "                'Question QIDs': question_qids,\n",
    "                'Answer QIDs': answer_qids,\n",
    "                'Question in Wikipedia': question_in_wikipedia,\n",
    "                'Answer in Wikipedia': answer_in_wikipedia,\n",
    "                'Question': data_chunk['question'],\n",
    "                'SPARQL': data_chunk['sparql_wikidata'],\n",
    "                'Ran SPARQL': ran_sparql\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(data_chunk)\n",
    "            raise e\n",
    "    return processed_rows\n",
    "\n",
    "# Use a list to accumulate the rows\n",
    "all_data = []\n",
    "\n",
    "main_dir = \"../data/Evaluation Data/LC_QuAD\"\n",
    "\n",
    "# Using ThreadPoolExecutor to parallelize file processing\n",
    "with ThreadPoolExecutor(max_workers=1) as executor:\n",
    "    futures = []\n",
    "    for file in tqdm(os.listdir(main_dir)):\n",
    "        file_path = os.path.join(main_dir, file)\n",
    "        futures.append(executor.submit(process_file, file_path))\n",
    "\n",
    "    for future in tqdm(futures):\n",
    "        all_data.extend(future.result())\n",
    "\n",
    "# Convert to DataFrame all at once at the end\n",
    "clean_data = pd.DataFrame(all_data)\n",
    "pickle.dump(clean_data, open(\"../data/Evaluation Data/LC_QuAD/processed_dataframe.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = pickle.load(open(\"../data/Evaluation Data/LC_QuAD/processed_dataframe.pkl\", \"rb\"))\n",
    "\n",
    "for i, row in tqdm(clean_data.iterrows()):\n",
    "    answer_qids = []\n",
    "    query = row['SPARQL']\n",
    "    if (not query.lower().strip().startswith('ask')) and (not row['Ran SPARQL']):\n",
    "        results = []\n",
    "        retry = 5\n",
    "        while retry > 0:  # Retry up to 5 times\n",
    "            try:\n",
    "                sparql.setQuery(query)\n",
    "                results = sparql.query().convert()[\"results\"][\"bindings\"]\n",
    "                retry = 0  # Exit loop if successful\n",
    "                clean_data.at[row.name, 'Ran SPARQL'] = True\n",
    "            except HTTPError as e:\n",
    "                print(e)\n",
    "                retry -= 1\n",
    "                time.sleep(1)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                retry -= 1\n",
    "                query = re.sub(r'LIMIT \\d+', 'LIMIT 5', query, flags=re.IGNORECASE)\n",
    "\n",
    "        for result in results:\n",
    "            for key in result:\n",
    "                value = result[key][\"value\"]\n",
    "                if 'www.wikidata.org' in result[key][\"value\"]:\n",
    "                    answer_qids.append(value.split('/')[-1])\n",
    "        answer_in_wikipedia = [is_in_wikipedia(id) for id in answer_qids]\n",
    "\n",
    "        clean_data.at[row.name, 'Answer QIDs'] = answer_qids\n",
    "        clean_data.at[row.name, 'Answer in Wikipedia'] = answer_in_wikipedia\n",
    "\n",
    "        if i%100 == 0:\n",
    "            pickle.dump(clean_data, open(\"../data/Evaluation Data/LC_QuAD/processed_dataframe.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a single file\n",
    "def process_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    processed_rows = []\n",
    "    for data_chunk in data:\n",
    "        correct_in_wikipedia = is_in_wikipedia(data_chunk['correct_id'])\n",
    "        wrong_in_wikipedia = is_in_wikipedia(data_chunk['wrong_id'])\n",
    "        processed_rows.append({\n",
    "            'Sentence': data_chunk['text'],\n",
    "            'Entity Name': data_chunk['string'],\n",
    "            'Correct QID': data_chunk['correct_id'],\n",
    "            'Wrong QID': data_chunk['wrong_id'],\n",
    "            'Correct in Wikipedia': correct_in_wikipedia,\n",
    "            'Wrong in Wikipedia': wrong_in_wikipedia,\n",
    "        })\n",
    "    return processed_rows\n",
    "\n",
    "# Define main directory and path to save results\n",
    "main_dir = \"../data/Evaluation Data/Wikidata-Disamb\"\n",
    "\n",
    "# Use a list to accumulate the rows\n",
    "all_data = []\n",
    "\n",
    "# Iterate over each file, process it, and save in chunks\n",
    "for file in tqdm(os.listdir(main_dir)):\n",
    "    file_path = os.path.join(main_dir, file)\n",
    "    processed_data = process_file(file_path)\n",
    "    all_data.extend(processed_data)\n",
    "\n",
    "# Convert to DataFrame all at once at the end\n",
    "clean_data = pd.DataFrame(all_data)\n",
    "pickle.dump(clean_data, open(\"../data/Evaluation Data/Wikidata-Disamb/processed_dataframe.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a single file\n",
    "def process_file(data, lang='en'):\n",
    "    processed_rows = []\n",
    "    for data_chunk in tqdm(data):\n",
    "        qid = data_chunk['uri']\n",
    "        qid_in_wikipedia = is_in_wikipedia(qid)\n",
    "        boundaries = [(e['start'], e['end']) for e in data_chunk['entities'] if e['uri'] == data_chunk['uri']]\n",
    "        processed_rows.append({\n",
    "            'Sentence': data_chunk['text'],\n",
    "            'Entity Name': data_chunk['title'],\n",
    "            'Entity Span': boundaries,\n",
    "            'Correct QID': qid,\n",
    "            'Correct in Wikipedia': qid_in_wikipedia,\n",
    "            'Language': data_chunk['lan']\n",
    "        })\n",
    "    return processed_rows\n",
    "\n",
    "# Use a list to accumulate the rows\n",
    "all_data = []\n",
    "\n",
    "huggingface_ds = load_dataset(\"Babelscape/REDFM\", \"all_languages\", streaming=True, trust_remote_code=True)\n",
    "\n",
    "for split in huggingface_ds:\n",
    "    all_data.extend(process_file(huggingface_ds[split]))\n",
    "\n",
    "# Convert to DataFrame all at once at the end\n",
    "clean_data = pd.DataFrame(all_data)\n",
    "pickle.dump(clean_data, open(\"../data/Evaluation Data/REDFM/processed_dataframe.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open(\"../data/Evaluation Data/REDFM/processed_dataframe.pkl\", \"rb\"))\n",
    "unique_ids = data['Correct QID'].unique()\n",
    "\n",
    "data = pickle.load(open(\"../data/Evaluation Data/Mintaka/processed_dataframe.pkl\", \"rb\"))\n",
    "unique_ids = np.concatenate([unique_ids, pd.Series(np.concatenate(data['Question QIDs'].to_numpy())).unique()])\n",
    "unique_ids = np.concatenate([unique_ids, pd.Series(np.concatenate(data['Answer QIDs'].to_numpy())).unique()])\n",
    "\n",
    "data = pickle.load(open(\"../data/Evaluation Data/KGConv/processed_dataframe.pkl\", \"rb\"))\n",
    "unique_ids = np.concatenate([unique_ids, data['Question QID'].unique()])\n",
    "unique_ids = np.concatenate([unique_ids, data['Answer QID'].unique()])\n",
    "\n",
    "data = pickle.load(open(\"../data/Evaluation Data/Wikidata-Disamb/processed_dataframe.pkl\", \"rb\"))\n",
    "unique_ids = np.concatenate([unique_ids, data['Correct QID'].unique()])\n",
    "unique_ids = np.concatenate([unique_ids, data['Wrong QID'].unique()])\n",
    "\n",
    "unique_ids = pd.Series(unique_ids).unique()\n",
    "unique_ids = pd.DataFrame({'QID': unique_ids})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "unique_ids['In Wikipedia'] = unique_ids['QID'].progress_apply(lambda x: (WikidataEntity.get_entity(x) is not None) and (WikidataID.get_id(x).in_wikipedia))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data  = pd.DataFrame(columns=['Query', 'Correct', 'Source'])\n",
    "\n",
    "data = pickle.load(open(\"../data/Evaluation Data/REDFM/processed_dataframe.pkl\", \"rb\"))\n",
    "prep = data[data['Language'] == 'en'][data['Correct in Wikipedia']]\n",
    "prep['Correct QID'] = prep['Correct QID'].apply(lambda x: [x])\n",
    "prep = prep.rename({'Sentence': 'Query', 'Correct QID': 'Correct'}, axis=1)\n",
    "prep['Source'] = 'REDFM'\n",
    "\n",
    "test_data = pd.concat([test_data, prep[['Query', 'Correct', 'Source']]])\n",
    "\n",
    "def mintaka_filter(x):\n",
    "    in_wiki = x['Question in Wikipedia'] + x['Answer in Wikipedia']\n",
    "    if len(in_wiki) == 0:\n",
    "        return False\n",
    "    return all(in_wiki)\n",
    "\n",
    "data = pickle.load(open(\"../data/Evaluation Data/Mintaka/processed_dataframe.pkl\", \"rb\"))\n",
    "prep = data[data.apply(lambda x: all(x['Question in Wikipedia'] + x['Answer in Wikipedia']), axis=1)]\n",
    "prep['Correct'] = prep.apply(lambda x: x['Question QIDs'] + x['Answer QIDs'], axis=1)\n",
    "prep = prep.rename({'Question': 'Query'}, axis=1)\n",
    "prep['Source'] = 'Mintaka'\n",
    "\n",
    "test_data = pd.concat([test_data, prep[['Query', 'Correct', 'Source']]])\n",
    "\n",
    "data = pickle.load(open(\"../data/Evaluation Data/KGConv/processed_dataframe.pkl\", \"rb\"))\n",
    "prep = data[data.apply(lambda x: x['Question in Wikipedia'] and x['Answer in Wikipedia'], axis=1)]\n",
    "prep['Correct'] = prep.apply(lambda x: [x['Question QID'], x['Answer QID']], axis=1)\n",
    "prep = prep.rename({'Question': 'Query'}, axis=1)\n",
    "prep['Source'] = 'KGConv'\n",
    "\n",
    "test_data = pd.concat([test_data, prep[['Query', 'Correct', 'Source']]])\n",
    "test_datacombined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_spans(sentence, spans, replace_with='Entity'):\n",
    "    # Sort spans in ascending order to remove from left to right\n",
    "    spans = sorted(spans, key=lambda x: x[0])\n",
    "    offset = 0  # To track the shift in index after replacing each span\n",
    "\n",
    "    for start, end in spans:\n",
    "        sentence = sentence[:start - offset] + replace_with + sentence[end - offset:]\n",
    "        offset += (end - start) - len(replace_with)  # Update offset to account for the replaced span length\n",
    "\n",
    "    return sentence\n",
    "\n",
    "data['Sentence no entity'] = data.apply(lambda x: remove_spans(x['Sentence'], x['Entity Span']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1351/1351 [2:42:37<00:00,  7.22s/it] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question QIDs</th>\n",
       "      <th>Answer QIDs</th>\n",
       "      <th>Question in Wikipedia</th>\n",
       "      <th>Answer in Wikipedia</th>\n",
       "      <th>Question</th>\n",
       "      <th>SPARQL</th>\n",
       "      <th>Language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Q127998, Q6256]</td>\n",
       "      <td>[Q219060]</td>\n",
       "      <td>[True, True]</td>\n",
       "      <td>[True]</td>\n",
       "      <td>Who is the  {country} for {head of state} of {...</td>\n",
       "      <td>select distinct ?sbj where { ?sbj wdt:P35 wd:...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Q1045]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[]</td>\n",
       "      <td>What was the population of Somalia in 2009-0-0?</td>\n",
       "      <td>SELECT ?obj WHERE { wd:Q1045 p:P1082 ?s . ?s p...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Q124057, Q3915489]</td>\n",
       "      <td>[Q4790397]</td>\n",
       "      <td>[True, True]</td>\n",
       "      <td>[True]</td>\n",
       "      <td>What is {nominated for} of {Dolores del Río} t...</td>\n",
       "      <td>SELECT ?obj WHERE { wd:Q124057 p:P1411 ?s . ?s...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Q42168]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[]</td>\n",
       "      <td>What was the population of Clermont-Ferrand on...</td>\n",
       "      <td>SELECT ?obj WHERE { wd:Q42168 p:P1082 ?s . ?s ...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Q3272]</td>\n",
       "      <td>[Q3292]</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[True]</td>\n",
       "      <td>On Lake Winnipeg what is the lakes on river?</td>\n",
       "      <td>select distinct ?answer where { ?answer wdt:P4...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81034</th>\n",
       "      <td>[Q18123741, Q9368]</td>\n",
       "      <td>[Q6853, Q154869, Q157661]</td>\n",
       "      <td>[True, True]</td>\n",
       "      <td>[True, True, True]</td>\n",
       "      <td>Nennen Sie eine Infektionskrankheit, die die L...</td>\n",
       "      <td>SELECT DISTINCT ?sbj ?sbj_label WHERE { ?sbj w...</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81035</th>\n",
       "      <td>[Q14982]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[True, True, True]</td>\n",
       "      <td>Der Siedepunkt des Methanols beträgt 117,6</td>\n",
       "      <td>ASK WHERE { wd:Q14982 wdt:P2102 ?obj filter(?o...</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81036</th>\n",
       "      <td>[Q295393]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Wie lautet der Professorenausweis (1909-1939) ...</td>\n",
       "      <td>select distinct ?answer where { wd:Q295393 wdt...</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81037</th>\n",
       "      <td>[Q42807, Q7118067]</td>\n",
       "      <td>[Q43301]</td>\n",
       "      <td>[True, True]</td>\n",
       "      <td>[True]</td>\n",
       "      <td>Welche Art von Menschen leben in Fresno, der P...</td>\n",
       "      <td>SELECT ?answer WHERE { wd:Q42807 wdt:P190 ?ans...</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81038</th>\n",
       "      <td>[Q313956, Q268200]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[True, True]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Für welchen Film hat Anil Kapoor einen Screen ...</td>\n",
       "      <td>select distinct ?sbj where { ?sbj wdt:P1346 w...</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81039 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Question QIDs                Answer QIDs Question in Wikipedia  \\\n",
       "0         [Q127998, Q6256]                  [Q219060]          [True, True]   \n",
       "1                  [Q1045]                         []                [True]   \n",
       "2      [Q124057, Q3915489]                 [Q4790397]          [True, True]   \n",
       "3                 [Q42168]                         []                [True]   \n",
       "4                  [Q3272]                    [Q3292]                [True]   \n",
       "...                    ...                        ...                   ...   \n",
       "81034   [Q18123741, Q9368]  [Q6853, Q154869, Q157661]          [True, True]   \n",
       "81035             [Q14982]                         []                [True]   \n",
       "81036            [Q295393]                         []                [True]   \n",
       "81037   [Q42807, Q7118067]                   [Q43301]          [True, True]   \n",
       "81038   [Q313956, Q268200]                         []          [True, True]   \n",
       "\n",
       "      Answer in Wikipedia                                           Question  \\\n",
       "0                  [True]  Who is the  {country} for {head of state} of {...   \n",
       "1                      []    What was the population of Somalia in 2009-0-0?   \n",
       "2                  [True]  What is {nominated for} of {Dolores del Río} t...   \n",
       "3                      []  What was the population of Clermont-Ferrand on...   \n",
       "4                  [True]       On Lake Winnipeg what is the lakes on river?   \n",
       "...                   ...                                                ...   \n",
       "81034  [True, True, True]  Nennen Sie eine Infektionskrankheit, die die L...   \n",
       "81035  [True, True, True]         Der Siedepunkt des Methanols beträgt 117,6   \n",
       "81036                  []  Wie lautet der Professorenausweis (1909-1939) ...   \n",
       "81037              [True]  Welche Art von Menschen leben in Fresno, der P...   \n",
       "81038                  []  Für welchen Film hat Anil Kapoor einen Screen ...   \n",
       "\n",
       "                                                  SPARQL Language  \n",
       "0       select distinct ?sbj where { ?sbj wdt:P35 wd:...       en  \n",
       "1      SELECT ?obj WHERE { wd:Q1045 p:P1082 ?s . ?s p...       en  \n",
       "2      SELECT ?obj WHERE { wd:Q124057 p:P1411 ?s . ?s...       en  \n",
       "3      SELECT ?obj WHERE { wd:Q42168 p:P1082 ?s . ?s ...       en  \n",
       "4      select distinct ?answer where { ?answer wdt:P4...       en  \n",
       "...                                                  ...      ...  \n",
       "81034  SELECT DISTINCT ?sbj ?sbj_label WHERE { ?sbj w...       de  \n",
       "81035  ASK WHERE { wd:Q14982 wdt:P2102 ?obj filter(?o...       de  \n",
       "81036  select distinct ?answer where { wd:Q295393 wdt...       de  \n",
       "81037  SELECT ?answer WHERE { wd:Q42807 wdt:P190 ?ans...       de  \n",
       "81038   select distinct ?sbj where { ?sbj wdt:P1346 w...       de  \n",
       "\n",
       "[81039 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "# prep = pickle.load(open(f\"../data/Evaluation Data/LC_QuAD/processed_dataframe.pkl\", \"rb\"))\n",
    "# prep['Language'] = 'en'\n",
    "\n",
    "target = 'de'\n",
    "translator = GoogleTranslator(source='en', target=target)\n",
    "\n",
    "batch_size = 20\n",
    "# translated_rows = []\n",
    "\n",
    "questions = prep[prep['Language'] == 'en']['Question'].tolist()\n",
    "\n",
    "# Process in batches\n",
    "for i in tqdm(range(len(translated_rows), len(questions), batch_size)):\n",
    "    batch = questions[i:i + batch_size]\n",
    "    translated_batch = translator.translate_batch(batch)\n",
    "\n",
    "    for idx, translated_question in enumerate(translated_batch):\n",
    "        original_row = prep.iloc[i + idx].copy()\n",
    "        original_row['Question'] = translated_question\n",
    "        original_row['Language'] = target\n",
    "        translated_rows.append(original_row)\n",
    "\n",
    "translated_df = pd.DataFrame(translated_rows)\n",
    "combined_df = pd.concat([prep, translated_df], ignore_index=True)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
