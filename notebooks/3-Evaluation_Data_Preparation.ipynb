{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and prepare the evaluation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import os\n",
    "os.environ[\"LANGUAGE\"] = 'ar'\n",
    "\n",
    "from sqlalchemy.sql.expression import func\n",
    "from wikidataDB import WikidataEntity, WikidataID, Session\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pickle\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "from requests.exceptions import HTTPError\n",
    "import time\n",
    "import numpy as np\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "def is_in_wikipedia(qid):\n",
    "    item = WikidataID.get_id(qid)\n",
    "    if item is None:\n",
    "        return False\n",
    "    return item.in_wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the KGConv dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    processed_rows = []\n",
    "    for _, data_chunks in data.items():\n",
    "        for data_chunk in data_chunks:\n",
    "            triple = data_chunk['triple']\n",
    "            question_in_wikipedia = is_in_wikipedia(triple[0])\n",
    "            answer_in_wikipedia = is_in_wikipedia(triple[2])\n",
    "            question = data_chunk['question variants'][0]\n",
    "            processed_rows.append({\n",
    "                'Question QID': triple[0],\n",
    "                'Property PID': triple[1],\n",
    "                'Answer QID': triple[2],\n",
    "                'Question in Wikipedia': question_in_wikipedia,\n",
    "                'Answer in Wikipedia': answer_in_wikipedia,\n",
    "                'Question': question['out-of-context'],\n",
    "                'Answer': data_chunk['answer']\n",
    "            })\n",
    "    return processed_rows\n",
    "\n",
    "all_data = []\n",
    "\n",
    "main_dir = \"../data/Evaluation Data/KGConv/complete_version\"\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "    futures = []\n",
    "    for folder in os.listdir(main_dir):\n",
    "        current_dir = os.path.join(main_dir, folder)\n",
    "        for file in tqdm(os.listdir(current_dir)):\n",
    "            file_path = os.path.join(current_dir, file)\n",
    "            futures.append(executor.submit(process_file, file_path))\n",
    "\n",
    "    for future in tqdm(futures):\n",
    "        all_data.extend(future.result())\n",
    "\n",
    "clean_data = pd.DataFrame(all_data)\n",
    "pickle.dump(clean_data, open(\"../data/Evaluation Data/KGConv/processed_dataframe.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the Mintaka dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    processed_rows = []\n",
    "    for data_chunk in data:\n",
    "        try:\n",
    "            question_qids = [d['name'] for d in data_chunk['questionEntity'] if d['entityType'] == 'entity']\n",
    "            question_in_wikipedia = [is_in_wikipedia(id) for id in question_qids]\n",
    "\n",
    "            answer_type = data_chunk['answer']['answerType']\n",
    "\n",
    "            answer_qids = []\n",
    "            answer_in_wikipedia = []\n",
    "            if (answer_type == 'entity') and (data_chunk['answer']['answer'] is not None):\n",
    "                answer_qids = [d['name'] for d in data_chunk['answer']['answer']]\n",
    "                answer_in_wikipedia = [is_in_wikipedia(id) for id in answer_qids]\n",
    "\n",
    "            processed_rows.append({\n",
    "                'Question QIDs': question_qids,\n",
    "                'Answer QIDs': answer_qids,\n",
    "                'Question in Wikipedia': question_in_wikipedia,\n",
    "                'Answer in Wikipedia': answer_in_wikipedia,\n",
    "                'Question': data_chunk['question'],\n",
    "                'Answer': data_chunk['answer']['mention'],\n",
    "                'Answer Type': answer_type,\n",
    "                'Language': 'en'\n",
    "            })\n",
    "\n",
    "            for lang in data_chunk['translations'].keys():\n",
    "                processed_rows.append({\n",
    "                    'Question QIDs': question_qids,\n",
    "                    'Answer QIDs': answer_qids,\n",
    "                    'Question in Wikipedia': question_in_wikipedia,\n",
    "                    'Answer in Wikipedia': answer_in_wikipedia,\n",
    "                    'Question': data_chunk['translations'][lang],\n",
    "                    'Answer': data_chunk['answer']['mention'],\n",
    "                    'Answer Type': answer_type,\n",
    "                    'Language': lang\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(data_chunk)\n",
    "            raise e\n",
    "    return processed_rows\n",
    "\n",
    "all_data = []\n",
    "\n",
    "main_dir = \"../data/Evaluation Data/Mintaka\"\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "    futures = []\n",
    "    for file in tqdm(os.listdir(main_dir)):\n",
    "        if 'json' in file:\n",
    "            file_path = os.path.join(main_dir, file)\n",
    "            futures.append(executor.submit(process_file, file_path))\n",
    "\n",
    "    for future in tqdm(futures):\n",
    "        all_data.extend(future.result())\n",
    "\n",
    "clean_data = pd.DataFrame(all_data)\n",
    "pickle.dump(clean_data, open(\"../data/Evaluation Data/Mintaka/processed_dataframe_full.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the RuBQ dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    processed_rows = []\n",
    "    for data_chunk in data:\n",
    "        try:\n",
    "            if data_chunk['question_uris'] is not None:\n",
    "                question_qids = [d.split('/')[-1] for d in data_chunk['question_uris']]\n",
    "                question_in_wikipedia = [is_in_wikipedia(id) for id in question_qids]\n",
    "\n",
    "                answers = [d['wd_names']['en'][0] if len(d['wd_names']['en']) > 0 else d['label'] for d in data_chunk['answers']]\n",
    "                answer_qids = [d['value'].split('/')[-1] for d in data_chunk['answers']]\n",
    "                answer_in_wikipedia = [is_in_wikipedia(id) for id in answer_qids]\n",
    "\n",
    "                processed_rows.append({\n",
    "                    'Question QIDs': question_qids,\n",
    "                    'Answer QIDs': answer_qids,\n",
    "                    'Question in Wikipedia': question_in_wikipedia,\n",
    "                    'Answer in Wikipedia': answer_in_wikipedia,\n",
    "                    'Question': data_chunk['question_eng'],\n",
    "                    'Answer': answers,\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(data_chunk)\n",
    "            raise e\n",
    "    return processed_rows\n",
    "\n",
    "all_data = []\n",
    "\n",
    "main_dir = \"../data/Evaluation Data/RuBQ\"\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "    futures = []\n",
    "    for file in tqdm(os.listdir(main_dir)):\n",
    "        file_path = os.path.join(main_dir, file)\n",
    "        futures.append(executor.submit(process_file, file_path))\n",
    "\n",
    "    for future in tqdm(futures):\n",
    "        all_data.extend(future.result())\n",
    "\n",
    "clean_data = pd.DataFrame(all_data)\n",
    "pickle.dump(clean_data, open(\"../data/Evaluation Data/RuBQ/processed_dataframe.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the LC_QuAD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sparql(query):\n",
    "    wikidata_endpoint = \"https://query.wikidata.org/sparql\"\n",
    "    sparql = SPARQLWrapper(wikidata_endpoint)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "\n",
    "    answer_qids = []\n",
    "    ran_sparql = True\n",
    "    if not query.lower().strip().startswith('ask'):\n",
    "        results = []\n",
    "        retry = 5\n",
    "        ran_sparql = False\n",
    "        while retry > 0:  # Retry up to 5 times\n",
    "            try:\n",
    "                sparql.setQuery(query)\n",
    "                results = sparql.query().convert()[\"results\"][\"bindings\"]\n",
    "                retry = 0  # Exit loop if successful\n",
    "                ran_sparql = True\n",
    "            except HTTPError as e:\n",
    "                print(e)\n",
    "                retry -= 1\n",
    "                time.sleep(1)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                retry -= 1\n",
    "                query = re.sub(r'LIMIT \\d+', 'LIMIT 5', query, flags=re.IGNORECASE) # Include a limit if the query returns too many answers\n",
    "\n",
    "        for result in results:\n",
    "            for key in result:\n",
    "                value = result[key][\"value\"]\n",
    "                if 'www.wikidata.org' in result[key][\"value\"]:\n",
    "                    answer_qids.append(value.split('/')[-1])\n",
    "\n",
    "    return answer_qids, ran_sparql\n",
    "\n",
    "def process_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    processed_rows = []\n",
    "    for data_chunk in data:\n",
    "        try:\n",
    "            matches = re.findall(r'wd:Q\\d+', data_chunk['sparql_wikidata'])\n",
    "            question_qids = [match[3:] for match in matches]\n",
    "            question_in_wikipedia = [is_in_wikipedia(id) for id in question_qids]\n",
    "\n",
    "            answer_qids, ran_sparql = run_sparql(data_chunk['sparql_wikidata'])\n",
    "            answer_in_wikipedia = [is_in_wikipedia(id) for id in answer_qids]\n",
    "\n",
    "            processed_rows.append({\n",
    "                'Question QIDs': question_qids,\n",
    "                'Answer QIDs': answer_qids,\n",
    "                'Question in Wikipedia': question_in_wikipedia,\n",
    "                'Answer in Wikipedia': answer_in_wikipedia,\n",
    "                'Question': data_chunk['question'],\n",
    "                'SPARQL': data_chunk['sparql_wikidata'],\n",
    "                'Ran SPARQL': ran_sparql\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(data_chunk)\n",
    "            raise e\n",
    "    return processed_rows\n",
    "\n",
    "all_data = []\n",
    "\n",
    "main_dir = \"../data/Evaluation Data/LC_QuAD\"\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=1) as executor:\n",
    "    futures = []\n",
    "    for file in tqdm(os.listdir(main_dir)):\n",
    "        file_path = os.path.join(main_dir, file)\n",
    "        futures.append(executor.submit(process_file, file_path))\n",
    "\n",
    "    for future in tqdm(futures):\n",
    "        all_data.extend(future.result())\n",
    "\n",
    "clean_data = pd.DataFrame(all_data)\n",
    "pickle.dump(clean_data, open(\"../data/Evaluation Data/LC_QuAD/processed_dataframe.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the Wikidata-Disamb dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    processed_rows = []\n",
    "    for data_chunk in data:\n",
    "        correct_in_wikipedia = is_in_wikipedia(data_chunk['correct_id'])\n",
    "        wrong_in_wikipedia = is_in_wikipedia(data_chunk['wrong_id'])\n",
    "        processed_rows.append({\n",
    "            'Sentence': data_chunk['text'],\n",
    "            'Entity Name': data_chunk['string'],\n",
    "            'Correct QID': data_chunk['correct_id'],\n",
    "            'Wrong QID': data_chunk['wrong_id'],\n",
    "            'Correct in Wikipedia': correct_in_wikipedia,\n",
    "            'Wrong in Wikipedia': wrong_in_wikipedia,\n",
    "        })\n",
    "    return processed_rows\n",
    "\n",
    "main_dir = \"../data/Evaluation Data/Wikidata-Disamb\"\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for file in tqdm(os.listdir(main_dir)):\n",
    "    file_path = os.path.join(main_dir, file)\n",
    "    processed_data = process_file(file_path)\n",
    "    all_data.extend(processed_data)\n",
    "\n",
    "clean_data = pd.DataFrame(all_data)\n",
    "pickle.dump(clean_data, open(\"../data/Evaluation Data/Wikidata-Disamb/processed_dataframe.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the REDFM dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(data):\n",
    "    processed_rows = []\n",
    "    for data_chunk in tqdm(data):\n",
    "        qid = data_chunk['uri']\n",
    "        qid_in_wikipedia = is_in_wikipedia(qid)\n",
    "        boundaries = [(e['start'], e['end']) for e in data_chunk['entities'] if e['uri'] == data_chunk['uri']]\n",
    "        processed_rows.append({\n",
    "            'Sentence': data_chunk['text'],\n",
    "            'Entity Name': data_chunk['title'],\n",
    "            'Entity Span': boundaries,\n",
    "            'Correct QID': qid,\n",
    "            'Correct in Wikipedia': qid_in_wikipedia,\n",
    "            'Language': data_chunk['lan']\n",
    "        })\n",
    "    return processed_rows\n",
    "\n",
    "all_data = []\n",
    "\n",
    "huggingface_ds = load_dataset(\"Babelscape/REDFM\", \"all_languages\", streaming=True, trust_remote_code=True)\n",
    "\n",
    "for split in huggingface_ds:\n",
    "    all_data.extend(process_file(huggingface_ds[split]))\n",
    "\n",
    "clean_data = pd.DataFrame(all_data)\n",
    "\n",
    "def remove_spans(sentence, spans, replace_with='Entity'):\n",
    "    # Sort spans in ascending order to remove from left to right\n",
    "    spans = sorted(spans, key=lambda x: x[0])\n",
    "    offset = 0  # To track the shift in index after replacing each span\n",
    "\n",
    "    for start, end in spans:\n",
    "        sentence = sentence[:start - offset] + replace_with + sentence[end - offset:]\n",
    "        offset += (end - start) - len(replace_with)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "clean_data['Sentence no entity'] = clean_data.apply(lambda x: remove_spans(x['Sentence'], x['Entity Span']), axis=1)\n",
    "\n",
    "pickle.dump(clean_data, open(\"../data/Evaluation Data/REDFM/processed_dataframe.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract all QIDs found in the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "data = pickle.load(open(\"../data/Evaluation Data/REDFM/processed_dataframe.pkl\", \"rb\"))\n",
    "unique_ids = data['Correct QID'].unique()\n",
    "\n",
    "data = pickle.load(open(\"../data/Evaluation Data/Mintaka/processed_dataframe.pkl\", \"rb\"))\n",
    "unique_ids = np.concatenate([unique_ids, pd.Series(np.concatenate(data['Question QIDs'].to_numpy())).unique()])\n",
    "unique_ids = np.concatenate([unique_ids, pd.Series(np.concatenate(data['Answer QIDs'].to_numpy())).unique()])\n",
    "\n",
    "data = pickle.load(open(\"../data/Evaluation Data/LC_QuAD/processed_dataframe.pkl\", \"rb\"))\n",
    "unique_ids = np.concatenate([unique_ids, pd.Series(np.concatenate(data['Question QIDs'].to_numpy())).unique()])\n",
    "unique_ids = np.concatenate([unique_ids, pd.Series(np.concatenate(data['Answer QIDs'].to_numpy())).unique()])\n",
    "\n",
    "data = pickle.load(open(\"../data/Evaluation Data/RuBQ/processed_dataframe.pkl\", \"rb\"))\n",
    "unique_ids = np.concatenate([unique_ids, pd.Series(np.concatenate(data['Question QIDs'].to_numpy())).unique()])\n",
    "unique_ids = np.concatenate([unique_ids, pd.Series(np.concatenate(data['Answer QIDs'].to_numpy())).unique()])\n",
    "\n",
    "data = pickle.load(open(\"../data/Evaluation Data/KGConv/processed_dataframe.pkl\", \"rb\"))\n",
    "unique_ids = np.concatenate([unique_ids, data['Question QID'].unique()])\n",
    "unique_ids = np.concatenate([unique_ids, data['Answer QID'].unique()])\n",
    "\n",
    "data = pickle.load(open(\"../data/Evaluation Data/Wikidata-Disamb/processed_dataframe.pkl\", \"rb\"))\n",
    "unique_ids = np.concatenate([unique_ids, data['Correct QID'].unique()])\n",
    "unique_ids = np.concatenate([unique_ids, data['Wrong QID'].unique()])\n",
    "\n",
    "unique_ids = pd.Series(unique_ids).unique()\n",
    "unique_ids = pd.DataFrame({'QID': unique_ids})\n",
    "\n",
    "unique_ids['In Wikipedia'] = unique_ids['QID'].progress_apply(lambda x: (WikidataEntity.get_entity(x) is not None) and (WikidataID.get_id(x).in_wikipedia))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a sample QIDs dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ids = unique_ids[unique_ids['In Wikipedia']]\n",
    "\n",
    "sample_count = sample_ids['from Evaluation'].sum()*2 - (~sample_ids['from Evaluation']).sum()\n",
    "with tqdm(total=sample_count) as progressbar:\n",
    "    with Session() as session:\n",
    "        entities = (\n",
    "            session.query(WikidataID)\n",
    "            .filter(WikidataID.in_wikipedia == True)\n",
    "            .order_by(func.random())  # Adds random ordering\n",
    "            .yield_per(1000)\n",
    "        )\n",
    "\n",
    "        for entity in tqdm(entities):\n",
    "            if entity.id not in sample_ids['QID'].values:\n",
    "                sample_ids = pd.concat([sample_ids, pd.DataFrame([{\n",
    "                        'QID': entity.id,\n",
    "                        'from Evaluation': False,\n",
    "                        'In Wikipedia': True,\n",
    "                    }])], ignore_index=True)\n",
    "                progressbar.update(1)\n",
    "            if progressbar.n >= sample_count:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Translate questions with Google Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open(f\"../data/Evaluation Data/LC_QuAD/processed_dataframe.pkl\", \"rb\"))\n",
    "data['Language'] = 'en'\n",
    "\n",
    "target = 'de'\n",
    "translator = GoogleTranslator(source='en', target=target)\n",
    "\n",
    "batch_size = 20\n",
    "translated_rows = []\n",
    "\n",
    "questions = data[data['Language'] == 'en']['Question'].tolist()\n",
    "\n",
    "# Process in batches\n",
    "for i in tqdm(range(len(translated_rows), len(questions), batch_size)):\n",
    "    batch = questions[i:i + batch_size]\n",
    "    translated_batch = translator.translate_batch(batch)\n",
    "\n",
    "    for idx, translated_question in enumerate(translated_batch):\n",
    "        original_row = data.iloc[i + idx].copy()\n",
    "        original_row['Question'] = translated_question\n",
    "        original_row['Language'] = target\n",
    "        translated_rows.append(original_row)\n",
    "\n",
    "translated_df = pd.DataFrame(translated_rows)\n",
    "combined_df = pd.concat([data, translated_df], ignore_index=True)\n",
    "combined_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
