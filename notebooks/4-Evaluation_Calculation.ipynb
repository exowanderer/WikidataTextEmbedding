{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mrr_score(prep, pred_col, true_cols):\n",
    "    # Remove duplicate QIDs while keeping the order\n",
    "    prep[pred_col] = prep[pred_col].apply(lambda x: list(dict.fromkeys(x)))\n",
    "    # Get the rank of each retrieved QID\n",
    "    ranks = prep.apply(lambda x: [i+1 for i in range(len(x[pred_col])) if (x[pred_col][i] in x[true_cols])], axis=1)\n",
    "    # Return the MRR\n",
    "    return ranks.apply(lambda x: 1/x[0] if len(x)>0 else 0).mean()\n",
    "\n",
    "def calculate_ndcg_score(prep, pred_col, true_cols):\n",
    "    # Remove duplicate QIDs while keeping the order\n",
    "    prep[pred_col] = prep[pred_col].apply(lambda x: list(dict.fromkeys(x)))\n",
    "    # Get the rank of each retrieved QID\n",
    "    ranks = prep.apply(lambda x: [i+1 for i in range(len(x[pred_col])) if (x[pred_col][i] in x[true_cols])], axis=1)\n",
    "    # Calculate the DCG, the Ideal DCG and finally return the NDCG\n",
    "    dcg = ranks.apply(lambda x: sum([1/np.log2(y+1) for y in x]) if len(x)>0 else 0)\n",
    "    idcg = prep.apply(lambda x: sum([1/np.log2(y+1) for y in range(1, min(len(x[true_cols]), len(x[pred_col])) + 1)]), axis=1)\n",
    "    return (dcg/idcg).mean()\n",
    "\n",
    "def calculate_accuracy_score(df):\n",
    "    highest_score_idx = df['Retrieval Score'].apply(np.argmax)\n",
    "    top_qid = df.apply(lambda x: x['Retrieval QIDs'][highest_score_idx[x.name]], axis=1)\n",
    "    return (top_qid == df['Correct QID']).mean()\n",
    "\n",
    "def calculate_log_odds_ratio_score(df):\n",
    "    def log_odds_ratio(row):\n",
    "        correct_qid = row['Correct QID']\n",
    "        wrong_qid = row['Wrong QID']\n",
    "\n",
    "        # Find the maximum scores for the correct and wrong QIDs\n",
    "        correct_scores = [score for qid, score in zip(row['Retrieval QIDs'], row['Retrieval Score']) if qid == correct_qid]\n",
    "        wrong_scores = [score for qid, score in zip(row['Retrieval QIDs'], row['Retrieval Score']) if qid == wrong_qid]\n",
    "\n",
    "        max_correct_score = max(correct_scores, default=0)\n",
    "        max_wrong_score = max(wrong_scores, default=0)\n",
    "\n",
    "        correct_log_odds = np.log(max_correct_score / (1 - max_correct_score))\n",
    "        wrong_log_odds = np.log(max_wrong_score / (1 - max_wrong_score))\n",
    "        return correct_log_odds - wrong_log_odds\n",
    "\n",
    "    # Apply the log odds ratio calculation to each row\n",
    "    return df.apply(log_odds_ratio, axis=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f\"../data/Evaluation Data/retrieval_results_Mintaka-wikidata_test_v1-en.pkl\"\n",
    "prep = pickle.load(open(filename, \"rb\"))\n",
    "assert ((pd.isna(prep['Retrieval QIDs']) | prep['Retrieval QIDs'].apply(lambda x: len(x) == 0)).sum() != 0), \"Evaluation not complete\"\n",
    "\n",
    "# For Mintaka, LC_QuAD, and RuBQ\n",
    "prep = prep[prep.apply(lambda x: all(x['Question in Wikipedia'] + x['Answer in Wikipedia']), axis=1)]\n",
    "prep['Correct QIDs'] = prep.apply(lambda x: x['Question QIDs'] + x['Answer QIDs'], axis=1)\n",
    "\n",
    "# For REDFM\n",
    "# prep = prep[prep['Correct in Wikipedia']]\n",
    "# prep['Correct QIDs'] = prep['Correct QID'].apply(lambda x: [x])\n",
    "\n",
    "print(\"Size Data: \", len(prep))\n",
    "print(\"MRR:\")\n",
    "print(calculate_mrr_score(prep, 'Retrieval QIDs', 'Correct QIDs'))\n",
    "print(\"NDCG:\")\n",
    "print(calculate_ndcg_score(prep, 'Retrieval QIDs', 'Correct QIDs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '../data/Evaluation Data/Language Results Balanced/REDFM-noentity'\n",
    "for file in os.listdir(directory):\n",
    "    print(file)\n",
    "    filename = f\"{directory}/{file}\"\n",
    "    prep = pickle.load(open(filename, \"rb\"))\n",
    "    if (pd.isna(prep['Retrieval QIDs']) | prep['Retrieval QIDs'].apply(lambda x: len(x) == 0)).sum() != 0:\n",
    "        print(\"Evaluation not complete\")\n",
    "        continue\n",
    "\n",
    "    # For Mintaka, LC_QuAD, and RuBQ\n",
    "    # prep = prep[prep.apply(lambda x: all(x['Question in Wikipedia'] + x['Answer in Wikipedia']), axis=1)]\n",
    "    # prep['Correct QIDs'] = prep.apply(lambda x: x['Question QIDs'] + x['Answer QIDs'], axis=1)\n",
    "\n",
    "    # For REDFM\n",
    "    prep = prep[prep['Correct in Wikipedia']]\n",
    "    prep['Correct QIDs'] = prep['Correct QID'].apply(lambda x: [x])\n",
    "\n",
    "    print(\"Size Data: \", len(prep))\n",
    "    print(\"MRR:\")\n",
    "    print(calculate_mrr_score(prep, 'Retrieval QIDs', 'Correct QIDs'))\n",
    "    print(\"NDCG:\")\n",
    "    print(calculate_ndcg_score(prep, 'Retrieval QIDs', 'Correct QIDs'))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f\"../data/Evaluation Data/retrieval_results_Wikidata-Disamb-wikidata_test_v1-en.pkl\"\n",
    "prep = pickle.load(open(filename, \"rb\"))\n",
    "assert ((pd.isna(prep['Retrieval QIDs']) | prep['Retrieval QIDs'].apply(lambda x: len(x) == 0)).sum() != 0), \"Evaluation not complete\"\n",
    "\n",
    "print(\"Size Data: \", len(prep))\n",
    "print(\"Accuracy:\")\n",
    "print(calculate_accuracy_score(prep))\n",
    "print(\"Log Odds:\")\n",
    "print(calculate_log_odds_ratio_score(prep))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
