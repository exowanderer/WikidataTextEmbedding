{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import os\n",
    "os.environ[\"LANGUAGE\"] = 'ar'\n",
    "\n",
    "from sqlalchemy.sql.expression import func\n",
    "from wikidataDB import WikidataEntity, WikidataID, Session\n",
    "from wikidataRetriever import WikidataKeywordSearch, AstraDBConnect\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pickle\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "from requests.exceptions import HTTPError\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def is_in_wikipedia(qid):\n",
    "    item = WikidataID.get_id(qid)\n",
    "    if item is None:\n",
    "        return False\n",
    "    return item.in_wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def calculate_mrr_score(df, pred_col, true_cols):\n",
    "    # Remove duplicate QIDs while keeping the order\n",
    "    prep[pred_col] = prep[pred_col].apply(lambda x: list(dict.fromkeys(x)))\n",
    "    # Get the rank of each retrieved QID\n",
    "    ranks = df.apply(lambda x: [i+1 for i in range(len(x[pred_col])) if (x[pred_col][i] in x[true_cols])], axis=1)\n",
    "    # Return the MRR\n",
    "    return ranks.apply(lambda x: 1/x[0] if len(x)>0 else 0).mean()\n",
    "\n",
    "def calculate_ndcg_score(df, pred_col, true_cols):\n",
    "    # Remove duplicate QIDs while keeping the order\n",
    "    prep[pred_col] = prep[pred_col].apply(lambda x: list(dict.fromkeys(x)))\n",
    "    # Get the rank of each retrieved QID\n",
    "    ranks = df.apply(lambda x: [i+1 for i in range(len(x[pred_col])) if (x[pred_col][i] in x[true_cols])], axis=1)\n",
    "    # Calculate the DCG, the Ideal DCG and finally return the NDCG\n",
    "    dcg = ranks.apply(lambda x: sum([1/np.log2(y+1) for y in x]) if len(x)>0 else 0)\n",
    "    idcg = df.apply(lambda x: sum([1/np.log2(y+1) for y in range(1, min(len(x[true_cols]), len(x[pred_col])) + 1)]), axis=1)\n",
    "    return (dcg/idcg).mean()\n",
    "\n",
    "collection = \"wikidata_test_v1\"\n",
    "evaluation_dataset = \"REDFM\"\n",
    "filename = f\"retrieval_results_{evaluation_dataset}-{collection}-arwithentity_DB-AR_EN_DE_Query-AR\"\n",
    "filename = f\"../data/Evaluation Data/retrieval_results_REDFM-wikidata_test_v1-DB(en,ar)-Query(EN)_DB-AR-EN_Query-EN.pkl\"\n",
    "\n",
    "# directory = '../data/Evaluation Data/Language Results/REDFM-noentity'\n",
    "# for file in os.listdir(directory):\n",
    "prep = pickle.load(open(filename, \"rb\"))\n",
    "assert pd.isna(prep['Retrieval QIDs']).sum() == 0, \"Evaluation not complete\"\n",
    "\n",
    "# For Mintaka, LC_QuAD, and RuBQ\n",
    "# prep = prep[prep.apply(lambda x: all(x['Question in Wikipedia'] + x['Answer in Wikipedia']), axis=1)]\n",
    "# prep['Correct QIDs'] = prep.apply(lambda x: x['Question QIDs'] + x['Answer QIDs'], axis=1)\n",
    "\n",
    "# For REDFM\n",
    "prep = prep[prep['Correct in Wikipedia']]\n",
    "prep['Correct QIDs'] = prep['Correct QID'].apply(lambda x: [x])\n",
    "\n",
    "print(file)\n",
    "print(\"MRR:\")\n",
    "print(calculate_mrr_score(prep, 'Retrieval QIDs', 'Correct QIDs'))\n",
    "print(\"NDCG:\")\n",
    "print(calculate_ndcg_score(prep, 'Retrieval QIDs', 'Correct QIDs'))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wikidataDB import WikidataEntity\n",
    "from wikidataEmbed import WikidataTextifier, JinaAIReranker\n",
    "\n",
    "collection = \"wikidata_test_v1\"\n",
    "evaluation_dataset = \"Mintaka\"\n",
    "filename = f\"retrieval_results_{evaluation_dataset}-{collection}-de_DB-EN_Query-DE\"\n",
    "prep = pickle.load(open(f\"../data/Evaluation Data/{filename}.pkl\", \"rb\"))\n",
    "\n",
    "textifier = WikidataTextifier(with_claim_aliases=False, with_property_aliases=False, language='en')\n",
    "reranker = JinaAIReranker()\n",
    "\n",
    "def rerank_qids(query, qids, reranker, textifier):\n",
    "    entities = [WikidataEntity.get_entity(qid) for qid in qids]\n",
    "    texts = [textifier.entity_to_text(entity) for entity in entities]\n",
    "    scores = reranker.rank(query, texts)\n",
    "\n",
    "    score_zip = zip(scores, prep.iloc[0]['Retrieval QIDs'])\n",
    "    score_zip = sorted(score_zip, key=lambda x: -x[0])\n",
    "    return [x[1] for x in score_zip]\n",
    "\n",
    "scores = rerank_qids(prep.iloc[0]['Question'], prep.iloc[0]['Retrieval QIDs'], reranker, textifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f\"retrieval_results_{evaluation_dataset}-{collection}-en_DB-EN_Query-EN\"\n",
    "prep = pickle.load(open(f\"../data/Evaluation Data/{filename}.pkl\", \"rb\"))\n",
    "prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "def calculate_accuracy_score(df):\n",
    "    highest_score_idx = df['Retrieval Score'].apply(np.argmax)\n",
    "    top_qid = df.apply(lambda x: x['Retrieval QIDs'][highest_score_idx[x.name]], axis=1)\n",
    "    return (top_qid == df['Correct QID']).mean()\n",
    "\n",
    "def calculate_log_odds_ratio_score(df):\n",
    "    def log_odds_ratio(row):\n",
    "        correct_qid = row['Correct QID']\n",
    "        wrong_qid = row['Wrong QID']\n",
    "\n",
    "        # Find the maximum scores for the correct and wrong QIDs\n",
    "        correct_scores = [score for qid, score in zip(row['Retrieval QIDs'], row['Retrieval Score']) if qid == correct_qid]\n",
    "        wrong_scores = [score for qid, score in zip(row['Retrieval QIDs'], row['Retrieval Score']) if qid == wrong_qid]\n",
    "\n",
    "        max_correct_score = max(correct_scores, default=0)\n",
    "        max_wrong_score = max(wrong_scores, default=0)\n",
    "\n",
    "        correct_log_odds = np.log(max_correct_score / (1 - max_correct_score))\n",
    "        wrong_log_odds = np.log(max_wrong_score / (1 - max_wrong_score))\n",
    "        return correct_log_odds - wrong_log_odds\n",
    "\n",
    "    # Apply the log odds ratio calculation to each row\n",
    "    return df.apply(log_odds_ratio, axis=1).mean()\n",
    "\n",
    "collection = \"wikidata_test_v2\"\n",
    "evaluation_dataset = \"Wikidata-Disamb\"\n",
    "prep = pickle.load(open(f\"../data/Evaluation Data/retrieval_results_{evaluation_dataset}-{collection}-en.pkl\", \"rb\"))\n",
    "assert pd.isna(prep['Retrieval QIDs']).sum() == 0, \"Evaluation not complete\"\n",
    "\n",
    "calculate_accuracy_score(prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_accuracy_over_K(df, pred_col, true_cols):\n",
    "    # Remove duplicate QIDs while keeping the order\n",
    "    prep[pred_col] = prep[pred_col].apply(lambda x: list(dict.fromkeys(x)))\n",
    "    # Get the rank of each retrieved QID\n",
    "    ranks = df.apply(lambda x: [i+1 for i in range(len(x[pred_col])) if (x[pred_col][i] in x[true_cols])], axis=1)\n",
    "    ranks = ranks.apply(lambda x: min(x) if len(x) > 0 else None)\n",
    "\n",
    "    accuracy = [(ranks <= i).mean() for i in range(int(ranks.max()))]\n",
    "    return accuracy\n",
    "\n",
    "collection = \"wikidata_test_v1\"\n",
    "evaluation_dataset = \"REDFM\"\n",
    "prep = pickle.load(open(f\"../data/Evaluation Data/retrieval_results_{evaluation_dataset}-{collection}-en.pkl\", \"rb\"))\n",
    "assert pd.isna(prep['Retrieval QIDs']).sum() == 0, \"Evaluation not complete\"\n",
    "prep = prep[prep['Correct in Wikipedia']]\n",
    "prep['Correct QIDs'] = prep['Correct QID'].apply(lambda x: [x])\n",
    "\n",
    "accuracy_v1 = calculate_accuracy_over_K(prep, 'Retrieval QIDs', 'Correct QIDs')\n",
    "\n",
    "collection = \"wikidata_test_v2\"\n",
    "evaluation_dataset = \"REDFM\"\n",
    "prep = pickle.load(open(f\"../data/Evaluation Data/retrieval_results_{evaluation_dataset}-{collection}-en.pkl\", \"rb\"))\n",
    "assert pd.isna(prep['Retrieval QIDs']).sum() == 0, \"Evaluation not complete\"\n",
    "prep = prep[prep['Correct in Wikipedia']]\n",
    "prep['Correct QIDs'] = prep['Correct QID'].apply(lambda x: [x])\n",
    "\n",
    "accuracy_v2 = calculate_accuracy_over_K(prep, 'Retrieval QIDs', 'Correct QIDs')\n",
    "\n",
    "# Create a simple bar chart\n",
    "plt.plot(list(range(len(accuracy_v1))), np.array(accuracy_v1)*100, label='Jina')\n",
    "plt.plot(list(range(len(accuracy_v2))), np.array(accuracy_v2)*100, label='Nvidia')\n",
    "plt.title('Accuracy of 1 correct item in REDFM')\n",
    "plt.xlabel('# Entities Retrieved')\n",
    "plt.ylabel('Accuracy %')\n",
    "plt.legend()\n",
    "\n",
    "# Show the chart\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.sql import func\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Modified query with random ordering\n",
    "sample_count = sample_ids['from Evaluation'].sum()*2 - (~sample_ids['from Evaluation']).sum()\n",
    "with tqdm(total=sample_count) as progressbar:\n",
    "    with Session() as session:\n",
    "        entities = (\n",
    "            session.query(WikidataID)\n",
    "            .filter(WikidataID.in_wikipedia == True)\n",
    "            .order_by(func.random())  # Adds random ordering\n",
    "            .yield_per(1000)\n",
    "        )\n",
    "\n",
    "        # Example of iterating through the entities\n",
    "        for entity in tqdm(entities):\n",
    "            if entity.id not in sample_ids['QID'].values:\n",
    "                sample_ids = pd.concat([sample_ids, pd.DataFrame([{\n",
    "                        'QID': entity.id,\n",
    "                        'from Evaluation': False,\n",
    "                        'In Wikipedia': True,\n",
    "                        'Sample 2': True\n",
    "                    }])], ignore_index=True)\n",
    "                progressbar.update(1)\n",
    "            if progressbar.n >= sample_count:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# prep = pickle.load(open(\"/home/philippe.saade/GitHub/WikidataTextEmbedding/data/Evaluation Data/KGConv/processed_dataframe.pkl\", \"rb\"))\n",
    "\n",
    "sample_ids = pickle.load(open(\"../data/Evaluation Data/Sample IDs (EN).pkl\", \"rb\"))\n",
    "sample_ids = sample_ids[sample_ids['In Wikipedia']]\n",
    "\n",
    "sample_qids_set = set(sample_ids['QID'].values)\n",
    "\n",
    "# Use vectorized operations for 'not_in_sample'\n",
    "# prep['Question in Wikipedia'] = prep['Question QID'].isin(sample_qids_set)\n",
    "# prep['Answer in Wikipedia'] = prep['Answer QID'].isin(sample_qids_set)\n",
    "# prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ids[sample_ids['Sample 2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in tqdm(prep.iterrows()):\n",
    "    for i in range(len(row['Answer QIDs'])):\n",
    "        if row['Answer in Wikipedia'][i] and row['Answer QIDs'][i] not in sample_qids_set:\n",
    "            sample_ids = pd.concat([sample_ids, pd.DataFrame([{\n",
    "                'QID': row['Answer QIDs'][i],\n",
    "                'from Evaluation': True,\n",
    "                'In Wikipedia': True,\n",
    "                'from Evaluation 2': True\n",
    "            }])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_spans(sentence, spans, replace_with='Entity'):\n",
    "    # Sort spans in ascending order to remove from left to right\n",
    "    spans = sorted(spans, key=lambda x: x[0])\n",
    "    offset = 0  # To track the shift in index after replacing each span\n",
    "\n",
    "    for start, end in spans:\n",
    "        sentence = sentence[:start - offset] + replace_with + sentence[end - offset:]\n",
    "        offset += (end - start) - len(replace_with)  # Update offset to account for the replaced span length\n",
    "\n",
    "    return sentence\n",
    "\n",
    "data['Sentence no entity'] = data.apply(lambda x: remove_spans(x['Sentence'], x['Entity Span']), axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
