{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGUAGE\"] = 'ar'\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from wikidataDB import Session, WikidataID, WikidataEntity\n",
    "from wikidataEmbed import WikidataTextifier\n",
    "from JinaAI import JinaAIEmbedder\n",
    "from wikidataRetriever import WikidataCirrusSeach, AstraDBConnect\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from langchain_core.documents import Document\n",
    "from ragstack_langchain.graph_store import CassandraGraphStore\n",
    "import cassio\n",
    "from langchain_astradb import AstraDBVectorStore, AstraDBGraphVectorStore\n",
    "from astrapy.info import CollectionVectorServiceOptions\n",
    "from astrapy import DataAPIClient\n",
    "from transformers import AutoTokenizer\n",
    "import requests\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ids = pickle.load(open(\"../data/Evaluation Data/Sample IDs (AR EN DE).pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastax_token = json.load(open(\"../API_tokens/datastax_wikidata_nvidia.json\"))\n",
    "\n",
    "COLLECTION_NAME = 'wikidata_test_v1'\n",
    "client = DataAPIClient(datastax_token['ASTRA_DB_APPLICATION_TOKEN'])\n",
    "database0 = client.get_database(datastax_token['ASTRA_DB_API_ENDPOINT'])\n",
    "wikiDataCollection = database0.get_collection(COLLECTION_NAME)\n",
    "\n",
    "COLLECTION_NAME = 'wikidata_langtest'\n",
    "graph_store = AstraDBConnect(datastax_token, COLLECTION_NAME, model='jina', batch_size=4, cache_embeddings=False)\n",
    "\n",
    "with tqdm(total=1347786) as progressbar:\n",
    "    for item in wikiDataCollection.find():\n",
    "        progressbar.update(1)\n",
    "        if item['metadata']['QID'] in sample_ids['QID'].values:\n",
    "            graph_store.add_document(id=item['_id'], text=item['content'], metadata=item['metadata'])\n",
    "\n",
    "    graph_store.push_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "OFFSET = 410\n",
    "LANGUAGE = 'de'\n",
    "textifier = WikidataTextifier(language=LANGUAGE)\n",
    "\n",
    "with tqdm(total=3226638) as progressbar:\n",
    "    with Session() as session:\n",
    "        entities = session.query(WikidataEntity).join(WikidataID, WikidataEntity.id == WikidataID.id).filter(WikidataID.in_wikipedia == True).offset(OFFSET).yield_per(BATCH_SIZE)\n",
    "        progressbar.update(OFFSET)\n",
    "\n",
    "        for entity in entities:\n",
    "            progressbar.update(1)\n",
    "            claims = entity.claims.copy()\n",
    "            for pid, claim in claims.items():\n",
    "                for c in claim:\n",
    "                    if ('datavalue' in c['mainsnak']):\n",
    "                        if (c['mainsnak']['datatype'] == 'quantity') or (c['mainsnak']['datatype'] == 'time'):\n",
    "                            c['mainsnak']['text_value'] = textifier.data_to_text(c['mainsnak']['datavalue'], c['mainsnak']['datatype'])\n",
    "\n",
    "                    if 'qualifiers' in c:\n",
    "                        for pid, qualifier in c['qualifiers'].items():\n",
    "                            for q in qualifier:\n",
    "                                if ('datavalue' in q):\n",
    "                                    if (q['datatype'] == 'quantity') or (q['datatype'] == 'time'):\n",
    "                                        q['text_value'] = textifier.data_to_text(q['datavalue'], q['datatype'])\n",
    "\n",
    "            entity.claims = claims\n",
    "            session.add(entity)\n",
    "            session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cassio\n",
    "from cassio.config import check_resolve_keyspace, check_resolve_session\n",
    "\n",
    "cassio.init(\n",
    "    token=datastax_token[\"ASTRA_DB_APPLICATION_TOKEN\"],\n",
    "    database_id=datastax_token[\"ASTRA_DB_DATABASE_ID\"],\n",
    "    keyspace=datastax_token[\"ASTRA_DB_KEYSPACE\"]\n",
    ")\n",
    "\n",
    "session = check_resolve_session()\n",
    "keyspace = check_resolve_keyspace()\n",
    "table = \"default_keyspace.wikidata_test_v2\"\n",
    "page_size = 1000\n",
    "# last_token = None\n",
    "\n",
    "with tqdm(total=530994) as progressbar:\n",
    "    while True:\n",
    "        # Construct query\n",
    "        if last_token is None:\n",
    "            query = f\"SELECT key, token(key) FROM {table} LIMIT {page_size};\"\n",
    "        else:\n",
    "            query = f\"SELECT key, token(key) FROM {table} WHERE token(key) > {last_token} LIMIT {page_size};\"\n",
    "\n",
    "        # Execute query\n",
    "        rows = session.execute(query)\n",
    "        rows_fetched = 0\n",
    "\n",
    "        # Process rows\n",
    "        for row in rows:\n",
    "            qid = row.key[1].split(\"_\")[0]\n",
    "            sample_ids.loc[sample_ids['QID'] == qid, 'in_wikidata_test_v2'] = True\n",
    "            last_token = row.system_token_key  # Update the last token\n",
    "            rows_fetched += 1\n",
    "            progressbar.update(1)\n",
    "\n",
    "        # Break if no more rows are fetched\n",
    "        if rows_fetched < page_size:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastax_token = json.load(open(\"../API_tokens/datastax_wikidata_nvidia.json\"))\n",
    "COLLECTION_NAME = 'wikidata_test_v1'\n",
    "\n",
    "client = DataAPIClient(datastax_token['ASTRA_DB_APPLICATION_TOKEN'])\n",
    "database0 = client.get_database(datastax_token['ASTRA_DB_API_ENDPOINT'])\n",
    "wikiDataCollection = database0.get_collection(COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    try:\n",
    "        for item in tqdm(wikiDataCollection.find({\n",
    "            '$not': {'metadata.Language': 'en'}\n",
    "        })):\n",
    "            metadata = item['metadata']\n",
    "            metadata['Language'] = 'en'\n",
    "            metadata[\"DumpDate\"]= '09/18/2024'\n",
    "            wikiDataCollection.update_one({'_id': item['_id']}, {'$set': {\"metadata\": metadata}})\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastax_token = json.load(open(\"../API_tokens/datastax_wikidata_nvidia.json\"))\n",
    "COLLECTION_NAME = 'wikidata_test_v1'\n",
    "\n",
    "client = DataAPIClient(datastax_token['ASTRA_DB_APPLICATION_TOKEN'])\n",
    "database0 = client.get_database(datastax_token['ASTRA_DB_API_ENDPOINT'])\n",
    "wikiDataCollection = database0.get_collection(COLLECTION_NAME)\n",
    "\n",
    "sample_ids = pickle.load(open(\"../data/Evaluation Data/Sample IDs (EN).pkl\", \"rb\"))\n",
    "sample_ids['in_wikidata_test_v1'] = False\n",
    "\n",
    "for qid in tqdm((sample_ids[~sample_ids['in_wikidata_test_v1']]['QID'].values)):\n",
    "  item = wikiDataCollection.find_one({\"_id\": f\"{qid}_1\"})\n",
    "  if item is not None:\n",
    "    sample_ids.loc[sample_ids['QID'] == qid, 'in_wikidata_test_v1'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import hashlib  # Import the hashlib library\n",
    "import json\n",
    "\n",
    "#OMIITED\n",
    "\n",
    "current_date = datetime.now().date().isoformat()\n",
    "\n",
    "#OMITTED\n",
    "\n",
    "def clean_claims(claims):\n",
    "    cleaned_claims = {}\n",
    "    for pid,value in claims.items():\n",
    "        cleaned_claims[pid] = [clean_item(item) for item in value]\n",
    "    return cleaned_claims\n",
    "def clean_item(item):\n",
    "    if 'datavalue' not in item['mainsnak']:\n",
    "        return {'type': item['mainsnak']['snaktype']}\n",
    "    if type(item['mainsnak']['datavalue']['value']) is dict:\n",
    "        value = {'type': item['mainsnak']['datavalue']['type'], **item['mainsnak']['datavalue']['value']}\n",
    "        if 'entity-type' in value:\n",
    "            del value['entity-type']\n",
    "        return value\n",
    "    return {'type': item['mainsnak']['datavalue']['type'], 'value': item['mainsnak']['datavalue']['value']}\n",
    "\n",
    "with tqdm(total=9203786) as progressbar:\n",
    "    with Session() as session:\n",
    "        entities = session.query(WikidataEntity).join(WikidataID, WikidataEntity.id == WikidataID.id).filter(WikidataID.in_wikipedia == True).offset(OFFSET).yield_per(BATCH_SIZE)\n",
    "        progressbar.update(OFFSET)\n",
    "        doc_batch = []\n",
    "        ids_batch = []\n",
    "\n",
    "        for entity in entities:\n",
    "            progressbar.update(1)\n",
    "            ##if SAMPLE and (entity.id in sample_ids['QID'].values):\n",
    "            chunks = textifier.chunk_text(entity, tokenizer, max_length=512)\n",
    "            for chunk_i in range(len(chunks)):\n",
    "                # Processing Claims\n",
    "                md5_hash = hashlib.md5(chunks[chunk_i].encode('utf-8')).hexdigest()\n",
    "                doc = Document(page_content=chunks[chunk_i],\n",
    "                                metadata={ \"MD5\": md5_hash,\n",
    "                                            \"Claims\": clean_claims(entity.claims),\n",
    "                                            \"Label\": entity.label,\n",
    "                                            \"Description\": entity.description,\n",
    "                                            \"Aliases\": entity.aliases,\n",
    "                                            \"Date\": current_date,\n",
    "                                            \"QID\": entity.id,\n",
    "                                            \"ChunkID\": chunk_i+1,\n",
    "                                            \"Language\": \"en\",\n",
    "                                            \"Dump Date\": \"09/18/2024\"})\n",
    "                doc_batch.append(doc)\n",
    "                ids_batch.append(f\"{entity.id}_{chunk_i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from wikidataDB import Session, WikidataID, WikidataEntity\n",
    "from wikidataEmbed import WikidataTextifier\n",
    "from wikidataRetriever import AstraDBConnect\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "\n",
    "SAMPLE = True\n",
    "BATCH_SIZE = 100\n",
    "OFFSET = 0\n",
    "LANGUAGE = 'en'\n",
    "DUMPDATE = \"09/18/2024\"\n",
    "datastax_token = json.load(open(\"../API_tokens/datastax_wikidata_nvidia.json\"))\n",
    "COLLECTION_NAME = 'wikidata_test_v2'\n",
    "\n",
    "textifier = WikidataTextifier(with_claim_aliases=False, with_property_aliases=False)\n",
    "graph_store = AstraDBConnect(datastax_token, COLLECTION_NAME, model='nvidia', batch_size=BATCH_SIZE, cache_embeddings=False)\n",
    "\n",
    "# Load the Sample IDs\n",
    "sample_ids = None\n",
    "if SAMPLE:\n",
    "    sample_ids = pickle.load(open(\"../data/Evaluation Data/Sample IDs (EN).pkl\", \"rb\"))\n",
    "    sample_ids = sample_ids[sample_ids['In Wikipedia']]\n",
    "    sample_ids = sample_ids[sample_ids['Sample 2']]\n",
    "\n",
    "with tqdm(total=len(sample_ids)) as progressbar:\n",
    "    for qid in range(0, len(sample_ids), 1000):\n",
    "        qid_list = sample_ids.iloc[qid:qid+1000]['QID'].tolist()\n",
    "        with Session() as session:\n",
    "            entities = session.query(WikidataEntity).filter(WikidataEntity.id.in_(qid_list)).all()\n",
    "\n",
    "        for entity in entities:\n",
    "            progressbar.update(1)\n",
    "            chunks = textifier.chunk_text(entity, graph_store.tokenizer, max_length=graph_store.max_token_size)\n",
    "            for chunk_i in range(len(chunks)):\n",
    "                metadata={\n",
    "                    \"QID\": entity.id,\n",
    "                    \"ChunkID\": chunk_i+1\n",
    "                }\n",
    "                graph_store.add_document(id=f\"{entity.id}_{chunk_i+1}\", text=chunks[chunk_i], metadata=metadata)\n",
    "    graph_store.push_batch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
