{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to embed and push the entities to the Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import os\n",
    "os.environ[\"LANGUAGE\"] = 'en' # Specify the language of the textified entities.\n",
    "\n",
    "from wikidataDB import Session, WikidataID, WikidataEntity\n",
    "from wikidataEmbed import WikidataTextifier\n",
    "from wikidataRetriever import AstraDBConnect\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "from astrapy import DataAPIClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = os.getenv(\"MODEL\", \"jina\")\n",
    "EMBED_BATCH_SIZE = int(os.getenv(\"EMBED_BATCH_SIZE\", 100))\n",
    "QUERY_BATCH_SIZE = int(os.getenv(\"QUERY_BATCH_SIZE\", 1000))\n",
    "OFFSET = int(os.getenv(\"OFFSET\", 0))\n",
    "API_KEY_FILENAME = os.getenv(\"API_KEY\", None)\n",
    "DUMPDATE = os.getenv(\"DUMPDATE\", '09/18/2024')\n",
    "\n",
    "COLLECTION_NAME = \"wikidata\"\n",
    "LANGUAGE = \"en\" # Language of the SQLite database\n",
    "TEXTIFIER_LANGUAGE = \"rdf\" # Language of the textifier (The name of the python script found in src/language_variables)\n",
    "\n",
    "API_KEY_FILENAME = os.listdir(\"../API_tokens/datastax_wikidata.json\")\n",
    "datastax_token = json.load(open(f\"../API_tokens/{API_KEY_FILENAME}\"))\n",
    "\n",
    "textifier = WikidataTextifier(language=TEXTIFIER_LANGUAGE)\n",
    "graph_store = AstraDBConnect(datastax_token, COLLECTION_NAME, model=MODEL, batch_size=EMBED_BATCH_SIZE, cache_embeddings=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Push Wikidata entities with QIDs in a sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ids = pickle.load(open(\"../data/Evaluation Data/Sample IDs (EN).pkl\", \"rb\"))\n",
    "sample_ids = sample_ids[sample_ids['In Wikipedia']]\n",
    "total_entities = len(sample_ids)\n",
    "\n",
    "def get_entity(session):\n",
    "    sample_qids = list(sample_ids['QID'].values)[OFFSET:]\n",
    "    sample_qid_batches = [sample_qids[i:i + QUERY_BATCH_SIZE] for i in range(0, len(sample_qids), QUERY_BATCH_SIZE)]\n",
    "\n",
    "    # For each batch of sample QIDs, fetch the entities from the database\n",
    "    for qid_batch in sample_qid_batches:\n",
    "        entities = session.query(WikidataEntity).filter(WikidataEntity.id.in_(qid_batch)).yield_per(QUERY_BATCH_SIZE)\n",
    "        for entity in entities:\n",
    "            yield entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm(total=total_entities-OFFSET) as progressbar:\n",
    "    with Session() as session:\n",
    "        entity_generator = get_entity(session)\n",
    "        doc_batch = []\n",
    "        ids_batch = []\n",
    "\n",
    "        for entity in entity_generator:\n",
    "            progressbar.update(1)\n",
    "            chunks = textifier.chunk_text(entity, graph_store.tokenizer, max_length=graph_store.max_token_size)\n",
    "            for chunk_i in range(len(chunks)):\n",
    "                md5_hash = hashlib.md5(chunks[chunk_i].encode('utf-8')).hexdigest()\n",
    "                metadata={\n",
    "                    \"MD5\": md5_hash,\n",
    "                    \"Label\": entity.label,\n",
    "                    \"Description\": entity.description,\n",
    "                    \"Aliases\": entity.aliases,\n",
    "                    \"Date\": datetime.now().isoformat(),\n",
    "                    \"QID\": entity.id,\n",
    "                    \"ChunkID\": chunk_i+1,\n",
    "                    \"Language\": LANGUAGE,\n",
    "                    \"DumpDate\": DUMPDATE\n",
    "                }\n",
    "                graph_store.add_document(id=f\"{entity.id}_{LANGUAGE}_{chunk_i+1}\", text=chunks[chunk_i], metadata=metadata)\n",
    "\n",
    "            tqdm.write(progressbar.format_meter(progressbar.n, progressbar.total, progressbar.format_dict[\"elapsed\"])) # tqdm is not wokring in docker compose. This is the alternative\n",
    "\n",
    "        graph_store.push_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Push all Wikidata entities found in the SQLite database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Session() as session:\n",
    "    total_entities = session.query(WikidataEntity).join(WikidataID, WikidataEntity.id == WikidataID.id).filter(WikidataID.in_wikipedia == True).offset(OFFSET).yield_per(QUERY_BATCH_SIZE)\n",
    "\n",
    "def get_entity(session):\n",
    "    entities = session.query(WikidataEntity).join(WikidataID, WikidataEntity.id == WikidataID.id).filter(WikidataID.in_wikipedia == True).offset(OFFSET).yield_per(QUERY_BATCH_SIZE)\n",
    "    for entity in entities:\n",
    "        yield entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm(total=total_entities-OFFSET) as progressbar:\n",
    "    with Session() as session:\n",
    "        entity_generator = get_entity(session)\n",
    "        doc_batch = []\n",
    "        ids_batch = []\n",
    "\n",
    "        for entity in entity_generator:\n",
    "            progressbar.update(1)\n",
    "            chunks = textifier.chunk_text(entity, graph_store.tokenizer, max_length=graph_store.max_token_size)\n",
    "            for chunk_i in range(len(chunks)):\n",
    "                md5_hash = hashlib.md5(chunks[chunk_i].encode('utf-8')).hexdigest()\n",
    "                metadata={\n",
    "                    \"MD5\": md5_hash,\n",
    "                    \"Label\": entity.label,\n",
    "                    \"Description\": entity.description,\n",
    "                    \"Aliases\": entity.aliases,\n",
    "                    \"Date\": datetime.now().isoformat(),\n",
    "                    \"QID\": entity.id,\n",
    "                    \"ChunkID\": chunk_i+1,\n",
    "                    \"Language\": LANGUAGE,\n",
    "                    \"DumpDate\": DUMPDATE\n",
    "                }\n",
    "                graph_store.add_document(id=f\"{entity.id}_{LANGUAGE}_{chunk_i+1}\", text=chunks[chunk_i], metadata=metadata)\n",
    "\n",
    "            tqdm.write(progressbar.format_meter(progressbar.n, progressbar.total, progressbar.format_dict[\"elapsed\"])) # tqdm is not wokring in docker compose. This is the alternative\n",
    "\n",
    "        graph_store.push_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy from one Astra collection to another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastax_token = json.load(open(\"../API_tokens/datastax_wikidata_nvidia.json\"))\n",
    "\n",
    "COLLECTION_NAME = 'wikidata_1'\n",
    "client = DataAPIClient(datastax_token['ASTRA_DB_APPLICATION_TOKEN'])\n",
    "database0 = client.get_database(datastax_token['ASTRA_DB_API_ENDPOINT'])\n",
    "wikiDataCollection = database0.get_collection(COLLECTION_NAME)\n",
    "\n",
    "COLLECTION_NAME = 'wikidata_2'\n",
    "graph_store = AstraDBConnect(datastax_token, COLLECTION_NAME, model='jina', batch_size=4, cache_embeddings=False)\n",
    "\n",
    "with tqdm(total=1347786) as progressbar:\n",
    "    for item in wikiDataCollection.find():\n",
    "        progressbar.update(1)\n",
    "        if item['metadata']['QID'] in sample_ids['QID'].values:\n",
    "            graph_store.add_document(id=item['_id'], text=item['content'], metadata=item['metadata'])\n",
    "\n",
    "    graph_store.push_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if all sample IDs are in Astra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastax_token = json.load(open(\"../API_tokens/datastax_wikidata_nvidia.json\"))\n",
    "COLLECTION_NAME = 'wikidata_1'\n",
    "\n",
    "client = DataAPIClient(datastax_token['ASTRA_DB_APPLICATION_TOKEN'])\n",
    "database0 = client.get_database(datastax_token['ASTRA_DB_API_ENDPOINT'])\n",
    "wikiDataCollection = database0.get_collection(COLLECTION_NAME)\n",
    "\n",
    "sample_ids = pickle.load(open(\"../data/Evaluation Data/Sample IDs (EN).pkl\", \"rb\"))\n",
    "sample_ids[f'in_{COLLECTION_NAME}'] = False\n",
    "\n",
    "for qid in tqdm((sample_ids[~sample_ids['in_wikidata_test_v1']]['QID'].values)):\n",
    "    item = wikiDataCollection.find_one({'metadata.QID': f'{qid}', 'metadata.Language': 'en'})\n",
    "    if item is not None:\n",
    "        sample_ids.loc[sample_ids['QID'] == qid, 'in_wikidata_test_v1'] = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
