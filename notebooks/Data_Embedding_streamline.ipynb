{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from wikidataDB import Session, WikidataID, WikidataEntity\n",
    "from wikidataEmbed import WikidataTextifier, JinaAIEmbeddings\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from langchain_core.documents import Document\n",
    "from ragstack_langchain.graph_store import CassandraGraphStore\n",
    "import cassio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "test_dataset = load_dataset(\"json\", data_files=\"test.json\", split=\"train\")\n",
    "\n",
    "test_dataset.save_to_disk(\"test.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastax_token = json.load(open(\"../API_tokens/datastax_token.json\"))\n",
    "os.environ[\"ASTRA_DB_DATABASE_ID\"] = datastax_token['database_id']\n",
    "os.environ[\"ASTRA_DB_APPLICATION_TOKEN\"] = datastax_token['token']\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "BATCH_LENGTH = 5000\n",
    "\n",
    "cassio.init(auto=True)\n",
    "embeddings = JinaAIEmbeddings(embedding_dim=1024)\n",
    "textifier = WikidataTextifier(with_claim_aliases=True, with_property_aliases=False)\n",
    "graph_store = CassandraGraphStore(\n",
    "    embeddings,\n",
    "    node_table=\"Wikidata_entities_v1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/Wikidata/pushed_embeddings.json\", \"r+\") as file:\n",
    "    json_str = file.read().strip()\n",
    "    if json_str.endswith(','):\n",
    "        json_str = json_str[:-1]+\"}\"\n",
    "    prev_data = json.loads(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = len([k for k,q in prev_data.items() if \"_1\" in k]) -5\n",
    "offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm(total=9203531) as progressbar:\n",
    "    with Session() as session:\n",
    "        entities = session.query(WikidataEntity).join(WikidataID, WikidataEntity.id == WikidataID.id).filter(WikidataID.in_wikipedia == True).offset(offset).yield_per(1000)\n",
    "        doc_batch = []\n",
    "        batch_length = 0\n",
    "        progressbar.update(offset)\n",
    "\n",
    "        for entity in entities:\n",
    "            progressbar.update(1)\n",
    "            chunks = embeddings.chunk_text(entity, textifier)\n",
    "            for chunk_i in range(len(chunks)):\n",
    "                doc = Document(page_content=chunks[chunk_i], metadata={\"QID\": entity.id, \"ChunkID\": chunk_i+1})\n",
    "                if f\"{entity.id}_{chunk_i+1}\" not in prev_data:\n",
    "                    doc_batch.append(doc)\n",
    "                    batch_length += len(chunks[chunk_i])\n",
    "\n",
    "                if batch_length >= BATCH_LENGTH:\n",
    "                    try:\n",
    "                        graph_store.add_documents(doc_batch)\n",
    "                        torch.cuda.empty_cache()\n",
    "                        with open(\"../data/Wikidata/pushed_embeddings.json\", \"a+\") as file:\n",
    "                            file.write(\", \".join([f\"\\\"{d.metadata[\"QID\"]}_{d.metadata[\"ChunkID\"]}\\\": 1\" for d in doc_batch]) +\", \")\n",
    "\n",
    "                        progressbar.set_description(f\"Batch Size: {len(doc_batch)}\")\n",
    "                        doc_batch = []\n",
    "                        batch_length = 0\n",
    "                    except Exception as e:\n",
    "                        torch.cuda.empty_cache()\n",
    "                        progressbar.set_description(f\"Batch Size: 1\")\n",
    "                        print(e)\n",
    "\n",
    "                        while len(doc_batch) > 0:\n",
    "                            doc = doc_batch.pop()\n",
    "                            embeddings.model = embeddings.model.to('cpu')\n",
    "                            graph_store.add_documents([doc])\n",
    "                            torch.cuda.empty_cache()\n",
    "                            with open(\"../data/Wikidata/pushed_embeddings.json\", \"a+\") as file:\n",
    "                                file.write(f\"\\\"{doc.metadata[\"QID\"]}_{doc.metadata[\"ChunkID\"]}\\\": 1, \")\n",
    "                        batch_length = 0\n",
    "                        embeddings.model = embeddings.model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_retriever = graph_store.as_retriever(search_kwargs={\"k\": 1000, \"depth\": 0})\n",
    "results = vector_retriever.get_relevant_documents(\"Is this a question?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cassio.init(auto=True)\n",
    "from cassio.config import check_resolve_keyspace, check_resolve_session\n",
    "\n",
    "session = check_resolve_session()\n",
    "keyspace = check_resolve_keyspace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.execute(f\"DROP TABLE IF EXISTS {keyspace}.wikidata_entities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from wikidataDB import Session, WikidataID, WikidataEntity\n",
    "from wikidataEmbed import WikidataTextifier\n",
    "\n",
    "import json\n",
    "from langchain_astradb import AstraDBVectorStore\n",
    "from langchain_core.documents import Document\n",
    "from astrapy.info import CollectionVectorServiceOptions\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from langchain_core.documents import Document\n",
    "import requests\n",
    "import time\n",
    "\n",
    "datastax_token = json.load(open(\"../API_tokens/datastax_wikidata_nvidia.json\"))\n",
    "ASTRA_DB_DATABASE_ID = datastax_token['ASTRA_DB_DATABASE_ID']\n",
    "ASTRA_DB_APPLICATION_TOKEN = datastax_token['ASTRA_DB_APPLICATION_TOKEN']\n",
    "ASTRA_DB_API_ENDPOINT = datastax_token[\"ASTRA_DB_API_ENDPOINT\"]\n",
    "ASTRA_DB_KEYSPACE = datastax_token[\"ASTRA_DB_KEYSPACE\"]\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "textifier = WikidataTextifier(with_claim_aliases=False, with_property_aliases=False)\n",
    "tokenizer = AutoTokenizer.from_pretrained('intfloat/e5-large-unsupervised', trust_remote_code=True, clean_up_tokenization_spaces=False)\n",
    "\n",
    "collection_vector_service_options = CollectionVectorServiceOptions(\n",
    "    provider=\"nvidia\",\n",
    "    model_name=\"NV-Embed-QA\"\n",
    ")\n",
    "\n",
    "graph_store = AstraDBVectorStore(\n",
    "    collection_name=\"wikidata\",\n",
    "    collection_vector_service_options=collection_vector_service_options,\n",
    "    token=ASTRA_DB_APPLICATION_TOKEN,\n",
    "    api_endpoint=ASTRA_DB_API_ENDPOINT,\n",
    "    namespace=ASTRA_DB_KEYSPACE,\n",
    ")\n",
    "\n",
    "vector_retriever = graph_store.as_retriever(search_kwargs={\"k\": 1000, \"depth\": 0})\n",
    "results = vector_retriever.get_relevant_documents(\"Is this a question?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cassio\n",
    "from cassio.config import check_resolve_keyspace, check_resolve_session\n",
    "\n",
    "cassio.init(database_id=ASTRA_DB_DATABASE_ID, token=ASTRA_DB_APPLICATION_TOKEN)\n",
    "session_cassio = check_resolve_session()\n",
    "keyspace = check_resolve_keyspace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 8342800\n",
    "with tqdm(total=9203531) as progressbar:\n",
    "    with Session() as session:\n",
    "        entities = session.query(WikidataEntity).join(WikidataID, WikidataEntity.id == WikidataID.id).filter(WikidataID.in_wikipedia == True).offset(offset).yield_per(BATCH_SIZE)\n",
    "        progressbar.update(offset)\n",
    "        n = 0\n",
    "        for entity in entities:\n",
    "            progressbar.update(1)\n",
    "            results = session_cassio.execute(f\"SELECT * FROM {ASTRA_DB_KEYSPACE}.wikidata WHERE key = (1, '{str(entity.id)}_1') LIMIT 1;\")\n",
    "            n += 1\n",
    "            if not results.one():\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 8342800\n",
    "with tqdm(total=9203531) as progressbar:\n",
    "    with Session() as session:\n",
    "        entities = session.query(WikidataEntity).join(WikidataID, WikidataEntity.id == WikidataID.id).filter(WikidataID.in_wikipedia == True).offset(offset).yield_per(BATCH_SIZE)\n",
    "        progressbar.update(offset)\n",
    "        doc_batch = []\n",
    "        ids_batch = []\n",
    "\n",
    "        for entity in entities:\n",
    "            progressbar.update(1)\n",
    "            chunks = textifier.chunk_text(entity, tokenizer)\n",
    "            for chunk_i in range(len(chunks)):\n",
    "                doc = Document(page_content=chunks[chunk_i], metadata={\"QID\": entity.id, \"ChunkID\": chunk_i+1})\n",
    "                doc_batch.append(doc)\n",
    "                ids_batch.append(f\"{entity.id}_{chunk_i+1}\")\n",
    "\n",
    "                if len(doc_batch) >= BATCH_SIZE:\n",
    "                    try:\n",
    "                        graph_store.add_documents(doc_batch, ids=ids_batch)\n",
    "                        doc_batch = []\n",
    "                        ids_batch = []\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        while True:\n",
    "                            try:\n",
    "                                response = requests.get(\"https://www.google.com\", timeout=5)\n",
    "                                if response.status_code == 200:\n",
    "                                    break\n",
    "                            except Exception as e:\n",
    "                                print(\"Waiting for internet connection...\")\n",
    "                                time.sleep(5)\n",
    "\n",
    "        if len(doc_batch) > 0:\n",
    "            graph_store.add_documents(doc_batch, ids=ids_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_store.add_documents(doc_batch, ids=ids_batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
