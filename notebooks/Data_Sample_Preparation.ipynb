{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from wikidataEmbed import WikidataTextifier\n",
    "from wikidataDB import WikidataEntity\n",
    "import pandas as pd\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract and clean-up entities from Wikidata API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_keys(data, keys_to_remove):\n",
    "    \"\"\"Removes all keys in a nested dictionary that are in the keys_to_remove list.\n",
    "\n",
    "    Args:\n",
    "        data (dict): Dictionary to process\n",
    "        keys_to_remove (list): A list of strings representing the keys to remove.\n",
    "\n",
    "    Returns:\n",
    "        dict: A cleaned-up dictionary.\n",
    "    \"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        data = {key: remove_keys(value, keys_to_remove) for key, value in data.items() if key not in keys_to_remove}\n",
    "    elif isinstance(data, list):\n",
    "        data = [remove_keys(item, keys_to_remove) for item in data]\n",
    "    return data\n",
    "\n",
    "def clean_datavalue(data):\n",
    "    \"\"\"Remove unnecessary nested arrays or dictionaries with one key. Keep keys that represent a Wikidata property or entity ID.\n",
    "\n",
    "    Args:\n",
    "        data (dict): Dictionary to process\n",
    "\n",
    "    Returns:\n",
    "        dict: A cleaned-up dictionary.\n",
    "    \"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        if (len(data.keys()) == 1) and not re.match(r\"^[PQ]\\d+$\", list(data.keys())[0]):\n",
    "            data = clean_datavalue(data[list(data.keys())[0]])\n",
    "        else:\n",
    "            data = {key: clean_datavalue(value) for key, value in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        data = [clean_datavalue(item) for item in data]\n",
    "    return data\n",
    "\n",
    "def get_labels(qid):\n",
    "    \"\"\"Get the labels of a Wikidata property or entity from the API.\n",
    "\n",
    "    Args:\n",
    "        qid (str): QID or PID of a Wikidata property or entity.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of labels in different languages.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        r = requests.get(f\"https://www.wikidata.org/w/api.php?action=wbgetentities&ids={qid}&format=json\")\n",
    "        entity = r.json()\n",
    "        return entity['entities'][qid]['labels']\n",
    "    except:\n",
    "        print(qid)\n",
    "        print(r.text)\n",
    "\n",
    "def add_labels(data):\n",
    "    \"\"\"Add the labels in the entity dictionary where they are missing. For example, for properties, and entities in claims...\n",
    "\n",
    "    Args:\n",
    "        data (_type_): Dictionary to process\n",
    "\n",
    "    Returns:\n",
    "        dict: The dictionary with the added labels\n",
    "    \"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        if 'property' in data:\n",
    "            data = {\n",
    "                **data,\n",
    "                'property-labels': get_labels(data['property'])\n",
    "            }\n",
    "        if ('unit' in data) and (data['unit'] != '1'):\n",
    "            data = {\n",
    "                **data,\n",
    "                'unit-labels': get_labels(data['unit'].split('/')[-1])\n",
    "            }\n",
    "        if ('datatype' in data) and ('datavalue' in data) and ((data['datatype'] == 'wikibase-item') or (data['datatype'] == 'wikibase-property')):\n",
    "            data['datavalue'] = {\n",
    "                'id': data['datavalue'],\n",
    "                'labels': get_labels(data['datavalue'])\n",
    "            }\n",
    "\n",
    "        data = {key: add_labels(value) for key, value in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        data = [add_labels(item) for item in data]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:04<00:00,  1.19it/s]\n"
     ]
    }
   ],
   "source": [
    "data_perentity = []\n",
    "for QID in tqdm(['Q2', 'Q42', 'Q90', 'Q5588', 'Q95']):\n",
    "    r = requests.get(f\"https://www.wikidata.org/wiki/Special:EntityData/{QID}.json\")\n",
    "    entity = r.json()['entities'][QID]\n",
    "\n",
    "    clean_claims = remove_keys(entity['claims'], ['hash', 'snaktype', 'type', 'entity-type', 'numeric-id', 'qualifiers-order', 'snaks-order'])\n",
    "    clean_claims = clean_datavalue(clean_claims)\n",
    "    clean_claims = remove_keys(clean_claims, ['id'])\n",
    "    clean_claims = add_labels(clean_claims)\n",
    "\n",
    "    clean_entity = {\n",
    "        'id': entity['id'],\n",
    "        'labels': entity['labels'],\n",
    "        'descriptions': entity['descriptions'],\n",
    "        'aliases': entity['aliases'],\n",
    "        'sitelinks': remove_keys(entity['sitelinks'], ['badges']),\n",
    "        'claims': clean_claims\n",
    "    }\n",
    "    data_perentity.append(clean_entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn the JSON into a table and spit the data by language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value(value):\n",
    "    value = value['datavalue'] if 'datavalue' in value else None\n",
    "    value = value['id'] if isinstance(value, dict) and ('id' in value) else value\n",
    "    value = value['time'] if isinstance(value, dict) and ('time' in value) else value\n",
    "    value = value['amount'] if isinstance(value, dict) and ('amount' in value) else value\n",
    "    return value\n",
    "\n",
    "def get_triplets(data):\n",
    "    \"\"\"Extract the triplets as QIDs and PIDs only. Include the value of the destination of the triplet if it's not an entity or property\n",
    "\n",
    "    Args:\n",
    "        data (dict): The extracted and cleaned-up entity.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries which include the property ID, and value.\n",
    "    \"\"\"\n",
    "    triplet_data = []\n",
    "    for pid,values in data['claims'].items():\n",
    "        for val in values:\n",
    "            triplet = {\"Property\": pid, \"Value\": get_value(val['mainsnak'])}\n",
    "\n",
    "            references = []\n",
    "            if 'references' in val:\n",
    "                for i in range(len(val['references'])):\n",
    "                    for ref_id, ref_values in val['references'][i].items():\n",
    "                        for ref_val in ref_values:\n",
    "                            references.append({\"Property\": ref_id, \"Value\": get_value(ref_val)})\n",
    "            triplet['references'] = references\n",
    "\n",
    "            qualifiers = []\n",
    "            if 'qualifiers' in val:\n",
    "                for qual_id, qual_values in val['qualifiers'].items():\n",
    "                    for qual_val in qual_values:\n",
    "                        qualifiers.append({\"Property\": qual_id, \"Value\": get_value(qual_val)})\n",
    "            triplet['qualifiers'] = qualifiers\n",
    "\n",
    "            triplet_data.append(triplet)\n",
    "    return triplet_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_perlang = []\n",
    "for d in data_perentity:\n",
    "    language = 'de'\n",
    "    textifier = WikidataTextifier(language=language)\n",
    "    qid = d['id']\n",
    "    entity = WikidataEntity.get_entity(qid)\n",
    "    content = textifier.entity_to_text(entity)\n",
    "    triplets = get_triplets(d)\n",
    "    data_perlang.append({\n",
    "        'id': qid,\n",
    "        'content': content,\n",
    "        'triplets': triplets,\n",
    "        'language': language,\n",
    "        'label': d['labels'].get(language, d['labels'].get('mul', {'value': ''}))['value'],\n",
    "        'description': d['descriptions'].get(language, d['descriptions'].get('mul', {'value': ''}))['value'],\n",
    "        'aliases': d['aliases'].get(language, []) + d['aliases'].get('mul', []),\n",
    "        'sitelinks': d['sitelinks']\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(data_perentity, open(\"wikidata_sample_perentity.json\", \"rb\"))\n",
    "data_perlang.to_excel('wikidata_sample_perlang.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
