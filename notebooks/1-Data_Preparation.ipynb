{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to read and process the Wikdata dump file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import os\n",
    "os.environ[\"LANGUAGE\"] = 'en' # Specify the language of the entity labels, description, and aliases.\n",
    "\n",
    "from wikidataDumpReader import WikidataDumpReader\n",
    "from wikidataDB import WikidataID, WikidataEntity, Session\n",
    "from sqlalchemy import select\n",
    "from multiprocessing import Manager\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the Wikidata dump ZIP file and saving the IDs of entities and properties to SQLite (Only the ones connected to the English Wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILEPATH = os.getenv(\"FILEPATH\", '../data/Wikidata/latest-all.json.bz2')\n",
    "BATCH_SIZE = int(os.getenv(\"BATCH_SIZE\", 10000))\n",
    "PUSH_SIZE = int(os.getenv(\"PUSH_SIZE\", 20000))\n",
    "QUEUE_SIZE = int(os.getenv(\"QUEUE_SIZE\", 15000))\n",
    "NUM_PROCESSES = int(os.getenv(\"NUM_PROCESSES\", 4))\n",
    "SKIPLINES = int(os.getenv(\"SKIPLINES\", 0))\n",
    "LANGUAGE = os.getenv(\"LANGUAGE\", 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_ids_to_sqlite(item, bulk_ids, sqlitDBlock):\n",
    "    if (item is not None) and WikidataID.is_in_wikipedia(item, language=LANGUAGE):\n",
    "        ids = WikidataID.extract_entity_ids(item, language=LANGUAGE)\n",
    "        bulk_ids.extend(ids)\n",
    "\n",
    "        with sqlitDBlock:\n",
    "            if len(bulk_ids) > PUSH_SIZE:\n",
    "                worked = WikidataID.add_bulk_ids(list(bulk_ids[:PUSH_SIZE]))\n",
    "                if worked:\n",
    "                    del bulk_ids[:PUSH_SIZE]\n",
    "\n",
    "multiprocess_manager = Manager()\n",
    "sqlitDBlock = multiprocess_manager.Lock()\n",
    "bulk_ids = multiprocess_manager.list()\n",
    "\n",
    "wikidata = WikidataDumpReader(FILEPATH, num_processes=NUM_PROCESSES, batch_size=BATCH_SIZE, queue_size=QUEUE_SIZE, skiplines=SKIPLINES)\n",
    "wikidata.run(lambda item: save_ids_to_sqlite(item, bulk_ids, sqlitDBlock), max_iterations=None, verbose=True)\n",
    "\n",
    "while len(bulk_ids) > 0:\n",
    "    worked = WikidataID.add_bulk_ids(list(bulk_ids))\n",
    "    if worked:\n",
    "        bulk_ids[:] = []\n",
    "    else:\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding entities (label, description, claims, and aliases) of IDs found in WikidataID to WikidataEntity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILEPATH = os.getenv(\"FILEPATH\", '../data/Wikidata/latest-all.json.bz2')\n",
    "BATCH_SIZE = int(os.getenv(\"BATCH_SIZE\", 1000))\n",
    "PUSH_SIZE = int(os.getenv(\"PUSH_SIZE\", 2000))\n",
    "QUEUE_SIZE = int(os.getenv(\"QUEUE_SIZE\", 1500))\n",
    "NUM_PROCESSES = int(os.getenv(\"NUM_PROCESSES\", 8))\n",
    "SKIPLINES = int(os.getenv(\"SKIPLINES\", 0))\n",
    "LANGUAGE = os.getenv(\"LANGUAGE\", 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_entities_to_sqlite(item, data_batch, sqlitDBlock):\n",
    "    if (item is not None) and WikidataID.get_id(item['id']):\n",
    "        item = WikidataEntity.normalise_item(item, language=LANGUAGE)\n",
    "        data_batch.append(item)\n",
    "\n",
    "        with sqlitDBlock:\n",
    "            if len(data_batch) > PUSH_SIZE:\n",
    "                worked = WikidataEntity.add_bulk_entities(list(data_batch[:PUSH_SIZE]))\n",
    "                if worked:\n",
    "                    del data_batch[:PUSH_SIZE]\n",
    "\n",
    "multiprocess_manager = Manager()\n",
    "sqlitDBlock = multiprocess_manager.Lock()\n",
    "data_batch = multiprocess_manager.list()\n",
    "\n",
    "wikidata = WikidataDumpReader(FILEPATH, num_processes=NUM_PROCESSES, batch_size=BATCH_SIZE, queue_size=QUEUE_SIZE, skiplines=SKIPLINES)\n",
    "wikidata.run(lambda item: save_entities_to_sqlite(item, bulk_ids, sqlitDBlock), max_iterations=None, verbose=True)\n",
    "\n",
    "while len(data_batch) > 0:\n",
    "    worked = WikidataEntity.add_bulk_entities(list(data_batch))\n",
    "    if worked:\n",
    "        data_batch[:] = []\n",
    "    else:\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find IDs that are in WikidataID but not in WikidataEntity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Session() as session:\n",
    "    result = session.execute(\n",
    "        select(WikidataID.id)\n",
    "        .outerjoin(WikidataEntity, WikidataID.id == WikidataEntity.id)\n",
    "        .filter(WikidataEntity.id == None)\n",
    "        .filter(WikidataID.in_wikipedia == True)\n",
    "    )\n",
    "    missing_ids = set(result.scalars().all())\n",
    "\n",
    "print(len(missing_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find IDs that are not in WikidataEntity but are in the claims, qualifiers, and quantity units of entities connected to Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missing_entities(session, ids):\n",
    "    existing_entities = session.query(WikidataEntity.id).filter(WikidataEntity.id.in_(ids)).all()\n",
    "    existing_ids = {entity.id for entity in existing_entities}\n",
    "    return set(ids) - existing_ids\n",
    "\n",
    "with Session() as session:\n",
    "    entities = session.query(WikidataEntity).join(WikidataID, WikidataEntity.id == WikidataID.id).filter(WikidataID.in_wikipedia == True).yield_per(100000)\n",
    "\n",
    "    progressbar = tqdm(total=9203531)\n",
    "    found = False\n",
    "    missing_ids = set()\n",
    "\n",
    "    batch_size = 10000\n",
    "    ids_to_check = set()\n",
    "\n",
    "    for entity in entities:\n",
    "        progressbar.update(1)\n",
    "        for pid, claim in entity.claims.items():\n",
    "            ids_to_check.add(pid)\n",
    "            for c in claim:\n",
    "                if ('datavalue' in c['mainsnak']):\n",
    "                    if ((c['mainsnak']['datatype'] == 'wikibase-item') or (c['mainsnak']['datatype'] == 'wikibase-property')):\n",
    "                        id = c['mainsnak']['datavalue']['value']['id']\n",
    "                        ids_to_check.add(id)\n",
    "                    elif (c['mainsnak']['datatype'] == 'quantity') and (c['mainsnak']['datavalue']['value']['unit'] != '1'):\n",
    "                        id = c['mainsnak']['datavalue']['value']['unit'].rsplit('/', 1)[1]\n",
    "                        ids_to_check.add(id)\n",
    "\n",
    "                if 'qualifiers' in c:\n",
    "                    for pid, qualifier in c['qualifiers'].items():\n",
    "                        ids_to_check.add(pid)\n",
    "                        for q in qualifier:\n",
    "                            if ('datavalue' in q):\n",
    "                                if ((q['datatype'] == 'wikibase-item') or (q['datatype'] == 'wikibase-property')):\n",
    "                                    id = q['datavalue']['value']['id']\n",
    "                                    ids_to_check.add(id)\n",
    "                                elif (q['datatype'] == 'quantity') and (q['datavalue']['value']['unit'] != '1'):\n",
    "                                    id = q['datavalue']['value']['unit'].rsplit('/', 1)[1]\n",
    "                                    ids_to_check.add(id)\n",
    "\n",
    "        if len(ids_to_check) >= batch_size:\n",
    "            missing_ids.update(get_missing_entities(session, ids_to_check))\n",
    "            ids_to_check.clear()\n",
    "\n",
    "        if progressbar.n % 1000 == 0:\n",
    "            progressbar.set_description(f\"Missing IDs: {len(missing_ids)}\")\n",
    "\n",
    "    if ids_to_check:\n",
    "        missing_ids.update(get_missing_entities(session, ids_to_check))\n",
    "\n",
    "    progressbar.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
