{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import os\n",
    "os.environ[\"LANGUAGE\"] = 'ar'\n",
    "\n",
    "from sqlalchemy.sql.expression import func\n",
    "from wikidataDB import WikidataEntity, WikidataID, Session\n",
    "from wikidataRetriever import WikidataKeywordSearch, AstraDBConnect\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pickle\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "from requests.exceptions import HTTPError\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def is_in_wikipedia(qid):\n",
    "    item = WikidataID.get_id(qid)\n",
    "    if item is None:\n",
    "        return False\n",
    "    return item.in_wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ids = pickle.load(open(\"../data/Evaluation Data/Sample IDs (EN).pkl\", \"rb\"))\n",
    "sample_ids = sample_ids[sample_ids['In Wikipedia']]\n",
    "sample_ids = set(sample_ids['QID'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4083442"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with Session() as session:\n",
    "    count = session.query(WikidataID.id).count()\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4083245"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with Session() as session:\n",
    "    count = session.query(WikidataEntity.id).count()\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastax_token = json.load(open(f\"../API_tokens/datastax_wikidata_nvidia.json\"))\n",
    "graph_store = AstraDBConnect(datastax_token, 'qids_nvidia', model='nvidia', batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1000\n",
    "with tqdm(total=9203786) as progressbar:\n",
    "    with Session() as session:\n",
    "        for i in range(2000000, 9203786, BATCH_SIZE):\n",
    "            entity = session.query(WikidataEntity).join(WikidataID, WikidataEntity.id == WikidataID.id).filter(WikidataID.in_wikipedia == True).offset(i).first()\n",
    "            progressbar.n = i\n",
    "            progressbar.refresh()\n",
    "            doc_batch = []\n",
    "            ids_batch = []\n",
    "            if graph_store.graph_store.get_by_document_id(f'{entity.id}_1') is None:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a single file\n",
    "def process_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    processed_rows = []\n",
    "    for _, data_chunks in data.items():\n",
    "        for data_chunk in data_chunks:\n",
    "            triple = data_chunk['triple']\n",
    "            question_in_wikipedia = is_in_wikipedia(triple[0])\n",
    "            answer_in_wikipedia = is_in_wikipedia(triple[2])\n",
    "            question = data_chunk['question variants'][0]\n",
    "            processed_rows.append({\n",
    "                'Question QID': triple[0],\n",
    "                'Property PID': triple[1],\n",
    "                'Answer QID': triple[2],\n",
    "                'Question in Wikipedia': question_in_wikipedia,\n",
    "                'Answer in Wikipedia': answer_in_wikipedia,\n",
    "                'Question': question['out-of-context'],\n",
    "                'Answer': data_chunk['answer']\n",
    "            })\n",
    "    return processed_rows\n",
    "\n",
    "# Use a list to accumulate the rows\n",
    "all_data = []\n",
    "\n",
    "main_dir = \"../data/Evaluation Data/KGConv/complete_version\"\n",
    "\n",
    "# Using ThreadPoolExecutor to parallelize file processing\n",
    "with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "    futures = []\n",
    "    for folder in os.listdir(main_dir):\n",
    "        current_dir = os.path.join(main_dir, folder)\n",
    "        for file in tqdm(os.listdir(current_dir)):\n",
    "            file_path = os.path.join(current_dir, file)\n",
    "            futures.append(executor.submit(process_file, file_path))\n",
    "\n",
    "    for future in tqdm(futures):\n",
    "        all_data.extend(future.result())\n",
    "\n",
    "# Convert to DataFrame all at once at the end\n",
    "clean_data = pd.DataFrame(all_data)\n",
    "pickle.dump(clean_data, open(\"../data/Evaluation Data/KGConv/processed_dataframe.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a single file\n",
    "def process_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    processed_rows = []\n",
    "    for data_chunk in data:\n",
    "        try:\n",
    "            question_qids = [d['name'] for d in data_chunk['questionEntity'] if d['entityType'] == 'entity']\n",
    "            question_in_wikipedia = [is_in_wikipedia(id) for id in question_qids]\n",
    "\n",
    "            answer_type = data_chunk['answer']['answerType']\n",
    "\n",
    "            answer_qids = []\n",
    "            answer_in_wikipedia = []\n",
    "            if (answer_type == 'entity') and (data_chunk['answer']['answer'] is not None):\n",
    "                answer_qids = [d['name'] for d in data_chunk['answer']['answer']]\n",
    "                answer_in_wikipedia = [is_in_wikipedia(id) for id in answer_qids]\n",
    "\n",
    "            processed_rows.append({\n",
    "                'Question QIDs': question_qids,\n",
    "                'Answer QIDs': answer_qids,\n",
    "                'Question in Wikipedia': question_in_wikipedia,\n",
    "                'Answer in Wikipedia': answer_in_wikipedia,\n",
    "                'Question': data_chunk['question'],\n",
    "                'Answer': data_chunk['answer']['mention'],\n",
    "                'Answer Type': answer_type,\n",
    "                'Language': 'en'\n",
    "            })\n",
    "\n",
    "            for lang in data_chunk['translations'].keys():\n",
    "                processed_rows.append({\n",
    "                    'Question QIDs': question_qids,\n",
    "                    'Answer QIDs': answer_qids,\n",
    "                    'Question in Wikipedia': question_in_wikipedia,\n",
    "                    'Answer in Wikipedia': answer_in_wikipedia,\n",
    "                    'Question': data_chunk['translations'][lang],\n",
    "                    'Answer': data_chunk['answer']['mention'],\n",
    "                    'Answer Type': answer_type,\n",
    "                    'Language': lang\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(data_chunk)\n",
    "            raise e\n",
    "    return processed_rows\n",
    "\n",
    "# Use a list to accumulate the rows\n",
    "all_data = []\n",
    "\n",
    "main_dir = \"../data/Evaluation Data/Mintaka\"\n",
    "\n",
    "# Using ThreadPoolExecutor to parallelize file processing\n",
    "with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "    futures = []\n",
    "    for file in tqdm(os.listdir(main_dir)):\n",
    "        if 'json' in file:\n",
    "            file_path = os.path.join(main_dir, file)\n",
    "            futures.append(executor.submit(process_file, file_path))\n",
    "\n",
    "    for future in tqdm(futures):\n",
    "        all_data.extend(future.result())\n",
    "\n",
    "# Convert to DataFrame all at once at the end\n",
    "clean_data = pd.DataFrame(all_data)\n",
    "pickle.dump(clean_data, open(\"../data/Evaluation Data/Mintaka/processed_dataframe_full.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a single file\n",
    "def process_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    processed_rows = []\n",
    "    for data_chunk in data:\n",
    "        try:\n",
    "            if data_chunk['question_uris'] is not None:\n",
    "                question_qids = [d.split('/')[-1] for d in data_chunk['question_uris']]\n",
    "                question_in_wikipedia = [is_in_wikipedia(id) for id in question_qids]\n",
    "\n",
    "                answers = [d['wd_names']['en'][0] if len(d['wd_names']['en']) > 0 else d['label'] for d in data_chunk['answers']]\n",
    "                answer_qids = [d['value'].split('/')[-1] for d in data_chunk['answers']]\n",
    "                answer_in_wikipedia = [is_in_wikipedia(id) for id in answer_qids]\n",
    "\n",
    "                processed_rows.append({\n",
    "                    'Question QIDs': question_qids,\n",
    "                    'Answer QIDs': answer_qids,\n",
    "                    'Question in Wikipedia': question_in_wikipedia,\n",
    "                    'Answer in Wikipedia': answer_in_wikipedia,\n",
    "                    'Question': data_chunk['question_eng'],\n",
    "                    'Answer': answers,\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(data_chunk)\n",
    "            raise e\n",
    "    return processed_rows\n",
    "\n",
    "# Use a list to accumulate the rows\n",
    "all_data = []\n",
    "\n",
    "main_dir = \"../data/Evaluation Data/RuBQ\"\n",
    "\n",
    "# Using ThreadPoolExecutor to parallelize file processing\n",
    "with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "    futures = []\n",
    "    for file in tqdm(os.listdir(main_dir)):\n",
    "        file_path = os.path.join(main_dir, file)\n",
    "        futures.append(executor.submit(process_file, file_path))\n",
    "\n",
    "    for future in tqdm(futures):\n",
    "        all_data.extend(future.result())\n",
    "\n",
    "# Convert to DataFrame all at once at the end\n",
    "clean_data = pd.DataFrame(all_data)\n",
    "pickle.dump(clean_data, open(\"../data/Evaluation Data/RuBQ/processed_dataframe.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidata_endpoint = \"https://query.wikidata.org/sparql\"\n",
    "sparql = SPARQLWrapper(wikidata_endpoint)\n",
    "sparql.setReturnFormat(JSON)\n",
    "\n",
    "sparql.setQuery(\"SELECT DISTINCT ?sbj ?sbj_label WHERE { ?sbj wdt:P31 wd:Q7187 . ?sbj rdfs:label ?sbj_label . FILTER(CONTAINS(lcase(?sbj_label), 'vgf')) . FILTER (lang(?sbj_label) = 'en') } LIMIT 25\")\n",
    "results = sparql.query().convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests.exceptions import HTTPError\n",
    "import time\n",
    "\n",
    "# Function to process a single file\n",
    "def process_file(file_path):\n",
    "    wikidata_endpoint = \"https://query.wikidata.org/sparql\"\n",
    "    sparql = SPARQLWrapper(wikidata_endpoint)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    processed_rows = []\n",
    "    for data_chunk in data:\n",
    "        try:\n",
    "            matches = re.findall(r'wd:Q\\d+', data_chunk['sparql_wikidata'])\n",
    "            question_qids = [match[3:] for match in matches]\n",
    "            question_in_wikipedia = [is_in_wikipedia(id) for id in question_qids]\n",
    "\n",
    "            answer_qids = []\n",
    "            query = data_chunk['sparql_wikidata']\n",
    "            ran_sparql = True\n",
    "            if not data_chunk['sparql_wikidata'].lower().strip().startswith('ask'):\n",
    "                results = []\n",
    "                retry = 5\n",
    "                ran_sparql = False\n",
    "                while retry > 0:  # Retry up to 5 times\n",
    "                    try:\n",
    "                        sparql.setQuery(query)\n",
    "                        results = sparql.query().convert()[\"results\"][\"bindings\"]\n",
    "                        retry = 0  # Exit loop if successful\n",
    "                        ran_sparql = True\n",
    "                    except HTTPError as e:\n",
    "                        print(e)\n",
    "                        retry -= 1\n",
    "                        time.sleep(1)\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        retry -= 1\n",
    "                        query = re.sub(r'LIMIT \\d+', 'LIMIT 5', query, flags=re.IGNORECASE)\n",
    "\n",
    "                for result in results:\n",
    "                    for key in result:\n",
    "                        value = result[key][\"value\"]\n",
    "                        if 'www.wikidata.org' in result[key][\"value\"]:\n",
    "                            answer_qids.append(value.split('/')[-1])\n",
    "                answer_in_wikipedia = [is_in_wikipedia(id) for id in answer_qids]\n",
    "\n",
    "            processed_rows.append({\n",
    "                'Question QIDs': question_qids,\n",
    "                'Answer QIDs': answer_qids,\n",
    "                'Question in Wikipedia': question_in_wikipedia,\n",
    "                'Answer in Wikipedia': answer_in_wikipedia,\n",
    "                'Question': data_chunk['question'],\n",
    "                'SPARQL': data_chunk['sparql_wikidata'],\n",
    "                'Ran SPARQL': ran_sparql\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(data_chunk)\n",
    "            raise e\n",
    "    return processed_rows\n",
    "\n",
    "# Use a list to accumulate the rows\n",
    "all_data = []\n",
    "\n",
    "main_dir = \"../data/Evaluation Data/LC_QuAD\"\n",
    "\n",
    "# Using ThreadPoolExecutor to parallelize file processing\n",
    "with ThreadPoolExecutor(max_workers=1) as executor:\n",
    "    futures = []\n",
    "    for file in tqdm(os.listdir(main_dir)):\n",
    "        file_path = os.path.join(main_dir, file)\n",
    "        futures.append(executor.submit(process_file, file_path))\n",
    "\n",
    "    for future in tqdm(futures):\n",
    "        all_data.extend(future.result())\n",
    "\n",
    "# Convert to DataFrame all at once at the end\n",
    "clean_data = pd.DataFrame(all_data)\n",
    "pickle.dump(clean_data, open(\"../data/Evaluation Data/LC_QuAD/processed_dataframe.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = pickle.load(open(\"../data/Evaluation Data/LC_QuAD/processed_dataframe.pkl\", \"rb\"))\n",
    "\n",
    "for i, row in tqdm(clean_data.iterrows()):\n",
    "    answer_qids = []\n",
    "    query = row['SPARQL']\n",
    "    if (not query.lower().strip().startswith('ask')) and (not row['Ran SPARQL']):\n",
    "        results = []\n",
    "        retry = 5\n",
    "        while retry > 0:  # Retry up to 5 times\n",
    "            try:\n",
    "                sparql.setQuery(query)\n",
    "                results = sparql.query().convert()[\"results\"][\"bindings\"]\n",
    "                retry = 0  # Exit loop if successful\n",
    "                clean_data.at[row.name, 'Ran SPARQL'] = True\n",
    "            except HTTPError as e:\n",
    "                print(e)\n",
    "                retry -= 1\n",
    "                time.sleep(1)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                retry -= 1\n",
    "                query = re.sub(r'LIMIT \\d+', 'LIMIT 5', query, flags=re.IGNORECASE)\n",
    "\n",
    "        for result in results:\n",
    "            for key in result:\n",
    "                value = result[key][\"value\"]\n",
    "                if 'www.wikidata.org' in result[key][\"value\"]:\n",
    "                    answer_qids.append(value.split('/')[-1])\n",
    "        answer_in_wikipedia = [is_in_wikipedia(id) for id in answer_qids]\n",
    "\n",
    "        clean_data.at[row.name, 'Answer QIDs'] = answer_qids\n",
    "        clean_data.at[row.name, 'Answer in Wikipedia'] = answer_in_wikipedia\n",
    "\n",
    "        if i%100 == 0:\n",
    "            pickle.dump(clean_data, open(\"../data/Evaluation Data/LC_QuAD/processed_dataframe.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a single file\n",
    "def process_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    processed_rows = []\n",
    "    for data_chunk in data:\n",
    "        correct_in_wikipedia = is_in_wikipedia(data_chunk['correct_id'])\n",
    "        wrong_in_wikipedia = is_in_wikipedia(data_chunk['wrong_id'])\n",
    "        processed_rows.append({\n",
    "            'Sentence': data_chunk['text'],\n",
    "            'Entity Name': data_chunk['string'],\n",
    "            'Correct QID': data_chunk['correct_id'],\n",
    "            'Wrong QID': data_chunk['wrong_id'],\n",
    "            'Correct in Wikipedia': correct_in_wikipedia,\n",
    "            'Wrong in Wikipedia': wrong_in_wikipedia,\n",
    "        })\n",
    "    return processed_rows\n",
    "\n",
    "# Define main directory and path to save results\n",
    "main_dir = \"../data/Evaluation Data/Wikidata-Disamb\"\n",
    "\n",
    "# Use a list to accumulate the rows\n",
    "all_data = []\n",
    "\n",
    "# Iterate over each file, process it, and save in chunks\n",
    "for file in tqdm(os.listdir(main_dir)):\n",
    "    file_path = os.path.join(main_dir, file)\n",
    "    processed_data = process_file(file_path)\n",
    "    all_data.extend(processed_data)\n",
    "\n",
    "# Convert to DataFrame all at once at the end\n",
    "clean_data = pd.DataFrame(all_data)\n",
    "pickle.dump(clean_data, open(\"../data/Evaluation Data/Wikidata-Disamb/processed_dataframe.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a single file\n",
    "def process_file(data, lang='en'):\n",
    "    processed_rows = []\n",
    "    for data_chunk in tqdm(data):\n",
    "        qid = data_chunk['uri']\n",
    "        qid_in_wikipedia = is_in_wikipedia(qid)\n",
    "        boundaries = [(e['start'], e['end']) for e in data_chunk['entities'] if e['uri'] == data_chunk['uri']]\n",
    "        processed_rows.append({\n",
    "            'Sentence': data_chunk['text'],\n",
    "            'Entity Name': data_chunk['title'],\n",
    "            'Entity Span': boundaries,\n",
    "            'Correct QID': qid,\n",
    "            'Correct in Wikipedia': qid_in_wikipedia,\n",
    "            'Language': data_chunk['lan']\n",
    "        })\n",
    "    return processed_rows\n",
    "\n",
    "# Use a list to accumulate the rows\n",
    "all_data = []\n",
    "\n",
    "huggingface_ds = load_dataset(\"Babelscape/REDFM\", \"all_languages\", streaming=True, trust_remote_code=True)\n",
    "\n",
    "for split in huggingface_ds:\n",
    "    all_data.extend(process_file(huggingface_ds[split]))\n",
    "\n",
    "# Convert to DataFrame all at once at the end\n",
    "clean_data = pd.DataFrame(all_data)\n",
    "pickle.dump(clean_data, open(\"../data/Evaluation Data/REDFM/processed_dataframe.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open(\"../data/Evaluation Data/REDFM/processed_dataframe.pkl\", \"rb\"))\n",
    "unique_ids = data['Correct QID'].unique()\n",
    "\n",
    "data = pickle.load(open(\"../data/Evaluation Data/Mintaka/processed_dataframe.pkl\", \"rb\"))\n",
    "unique_ids = np.concatenate([unique_ids, pd.Series(np.concatenate(data['Question QIDs'].to_numpy())).unique()])\n",
    "unique_ids = np.concatenate([unique_ids, pd.Series(np.concatenate(data['Answer QIDs'].to_numpy())).unique()])\n",
    "\n",
    "data = pickle.load(open(\"../data/Evaluation Data/KGConv/processed_dataframe.pkl\", \"rb\"))\n",
    "unique_ids = np.concatenate([unique_ids, data['Question QID'].unique()])\n",
    "unique_ids = np.concatenate([unique_ids, data['Answer QID'].unique()])\n",
    "\n",
    "data = pickle.load(open(\"../data/Evaluation Data/Wikidata-Disamb/processed_dataframe.pkl\", \"rb\"))\n",
    "unique_ids = np.concatenate([unique_ids, data['Correct QID'].unique()])\n",
    "unique_ids = np.concatenate([unique_ids, data['Wrong QID'].unique()])\n",
    "\n",
    "unique_ids = pd.Series(unique_ids).unique()\n",
    "unique_ids = pd.DataFrame({'QID': unique_ids})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "unique_ids['In Wikipedia'] = unique_ids['QID'].progress_apply(lambda x: (WikidataEntity.get_entity(x) is not None) and (WikidataID.get_id(x).in_wikipedia))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data  = pd.DataFrame(columns=['Query', 'Correct', 'Source'])\n",
    "\n",
    "data = pickle.load(open(\"../data/Evaluation Data/REDFM/processed_dataframe.pkl\", \"rb\"))\n",
    "prep = data[data['Language'] == 'en'][data['Correct in Wikipedia']]\n",
    "prep['Correct QID'] = prep['Correct QID'].apply(lambda x: [x])\n",
    "prep = prep.rename({'Sentence': 'Query', 'Correct QID': 'Correct'}, axis=1)\n",
    "prep['Source'] = 'REDFM'\n",
    "\n",
    "test_data = pd.concat([test_data, prep[['Query', 'Correct', 'Source']]])\n",
    "\n",
    "def mintaka_filter(x):\n",
    "    in_wiki = x['Question in Wikipedia'] + x['Answer in Wikipedia']\n",
    "    if len(in_wiki) == 0:\n",
    "        return False\n",
    "    return all(in_wiki)\n",
    "\n",
    "data = pickle.load(open(\"../data/Evaluation Data/Mintaka/processed_dataframe.pkl\", \"rb\"))\n",
    "prep = data[data.apply(lambda x: all(x['Question in Wikipedia'] + x['Answer in Wikipedia']), axis=1)]\n",
    "prep['Correct'] = prep.apply(lambda x: x['Question QIDs'] + x['Answer QIDs'], axis=1)\n",
    "prep = prep.rename({'Question': 'Query'}, axis=1)\n",
    "prep['Source'] = 'Mintaka'\n",
    "\n",
    "test_data = pd.concat([test_data, prep[['Query', 'Correct', 'Source']]])\n",
    "\n",
    "data = pickle.load(open(\"../data/Evaluation Data/KGConv/processed_dataframe.pkl\", \"rb\"))\n",
    "prep = data[data.apply(lambda x: x['Question in Wikipedia'] and x['Answer in Wikipedia'], axis=1)]\n",
    "prep['Correct'] = prep.apply(lambda x: [x['Question QID'], x['Answer QID']], axis=1)\n",
    "prep = prep.rename({'Question': 'Query'}, axis=1)\n",
    "prep['Source'] = 'KGConv'\n",
    "\n",
    "test_data = pd.concat([test_data, prep[['Query', 'Correct', 'Source']]])\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_spans(sentence, spans, replace_with='Entity'):\n",
    "    # Sort spans in ascending order to remove from left to right\n",
    "    spans = sorted(spans, key=lambda x: x[0])\n",
    "    offset = 0  # To track the shift in index after replacing each span\n",
    "\n",
    "    for start, end in spans:\n",
    "        sentence = sentence[:start - offset] + replace_with + sentence[end - offset:]\n",
    "        offset += (end - start) - len(replace_with)  # Update offset to account for the replaced span length\n",
    "\n",
    "    return sentence\n",
    "\n",
    "data['Sentence no entity'] = data.apply(lambda x: remove_spans(x['Sentence'], x['Entity Span']), axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
